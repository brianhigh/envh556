---
title: "ENVH 556 Week 1 Lab:  Becoming familiar with R, RStudio, RMarkdown"
author: "Lianne Sheppard for ENVH 556"
date: "Created Winter 2019; updated `r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

TODO:  Modify the loading of packages so ones loaded aren't reloaded.  Is this fixed now given we are using cache?

---

Note:  The following three chunks should typically appear at the beginning of each R Markdown file.  We display them during this first lab so you can see them in the output.  Ordinarily we would not show them.
```{r setup, include=FALSE, echo=TRUE}
#------------setup-------------
# this chunk won't execute when knitting because include=FALSE (hides everything)
# We are showing the code in this chunk in the first lab; ordinarily we will choose to hide it with the option echo=FALSE
# Note about naming chunks:  the chunk name is useful for reading code, for the index you can choose to show in RStudio to the right of the script editor, and for more advanced purposes.  The chunk name will not show up in your Appendix code compilation however.  The comment with the chunk name between dashes will both show up in the index (boldfaced) and print in your appendix.  This facilitates review of the code.  I recommend using chunk names and putting them at least as a comment at the beginning of the chunk.
##
knitr::opts_chunk$set(
    echo = TRUE,
    cache = TRUE,
    cache.comments = FALSE,
    message = FALSE,
    warning = FALSE
)
##TODO: check online; see help for opts_chunk{knitr}
##TODO: ADD other setup defaults including loading libraries PLUS other ENVH556 standards??

```

```{r clear.workspace, eval=FALSE, echo=TRUE}
#---------clear.workspace------------
## code to clear the environment without clearing knitr
## useful for code development because it simulates the knitr environment
## Run as a code chunk when testing.  When knitr is run, this is effectively run in the knitr environment

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```


```{r load.libraries.with.pacman, include=TRUE}
#-----load.libraries.with.pacman--------------
# Ordinarily we set include=FALSE for this chunk; just showing it for the first lab.
# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  Some reasons for packages:
# doBy: summaryBy  -- Don't load:  now replaced by tidyr
# reshape2:  melt() --Don't load:  now replaced by tidyr
# knitr:  kable()
# ggplot2: Don't load:  part of tidyverse
# readr:   Don't load:  part of tidyverse
# dplyr:   Don't load:  part of tidyverse  (replaces plyr -- don't load plyr!)
# Hmisc:  describe
# EnvStats: geoMean, geoSD, probability plotting functions
# TODO:  modify so it only reloads packages not already loaded (use cache??)
pacman::p_load(tidyverse, knitr, Hmisc, EnvStats)

```

---

# Goal
The goal of this lab is to help everyone become familiar with R, R Studio, and R Markdown through the context of the course content.  This is also your first opportunity to develop a lab write-up for ENVH 556 using R Markdown.

Note:  It is in your best interest to learn modern R tools, such as those included in `tidyverse`.  Our goal in this class is to prioritize these tools as much as possible while recognizing that there are many ways to accomplish the same task and we don't want to get too bogged down in the best way to accomplish our goals.

---

# Practice Session
This section covers basic practice to be completed during the lab.  It will introduce you to multiple useful commands and help you get a basic understanding of the dataset.

The underlying data to be used in the lab is from the DEMS study and is described in Coble et al 2010.  We are focusing on the personal data collected by NIOSH between 1998 and 2001.  In this lab we will mostly use the `ecdata` variable which is a measurement of exposure to respirable elemental carbon, also called REC.  The data were collected from workers from a cross-section of jobs at the seven mines open during the data collection.  For further information on this dataset, see the document: *DEMS Personal Data overview* available on the class Canvas site.

A big part of data science is data management and ideally you will also learn basic data management in conjunction with your work.  This class will not emphasize data management, although you will do some during your term project.   Note that some of the habits we stress, including those we discuss in this lab, are very useful for both data management and data analyses.


## I. Set up a RStudio project, file paths, and read the data

Complete the following steps to set up the lab  <!--Note: need a new line for R Markdown to recognize the following is a bulleted list -->

* Start a new R project in R Studio.  
    + Note:  Put your project for this lab in a sensible directory.  
    + Create a subdirectory of that directory with a name such as `Datasets` and copy the R dataset *DEMSCombinedPersonal.rds* into it. (This lab assumes your data directory is named `Datasets`.)
    + Copy this file, *Week1Lab.Rmd* into your project directory and open it into RStudio.
    + Note:  Often you will open a new .Rmd (R Markdown) file and populate it with code and text for this course, such as from the *LabReportGuidelines.Rmd* file.  

* In the R Markdown file you are using for this lab make sure you have your file paths set up.  
    i) First assign your current path to a variable name (see *getwd* chunk).  
    ii) Then tell R where your data reside (see *datasets.path* chunk).  The `dir.create()` command will make a directory if it doesn't exist.  The `file.path()` command allows you to refer easily to paths across operating systems.
    iii) See the following chunks to accomplish this:
    
<!--Note: to get this list to work correctly had to put ii) before the following R chunks-->
```{r getwd}
#-----------getwd-----------------
    ProjectPath <- getwd()
    #ProjectPath
```
   
```{r datasets.path}
#------------datasets.path----------------
    dir.create(file.path(ProjectPath,"Datasets"),     showWarnings=FALSE, recursive = TRUE)
    datapath<-file.path(ProjectPath,"Datasets")
```
* Read in the data 
     + The data we will use have already been converted to an R dataset.  Make sure the "DEMSCombinedPersonal.rds" dataset is in your Datasets directory defined by `datapath`.
```{r read.data, eval=TRUE}
#---------------read.data------------------------
# Note:  for simplicity of typing in this lab we're calling the dataset DEMS, even though it is only one of several DEMS datasets we will use in this course.
DEMS<-readRDS(file.path(datapath,"DEMSCombinedPersonal.rds"))
```


## II. Check your data first!

An essential part of any data analysis is making sure the data you are analyzing are as you expect.  Before you start any new data analysis, make sure your data have been read in correctly and get a basic understanding of their structure.

###1. Check to make sure your data have been read in correctly 
Here are some basic questions you should ask every time you read in a new dataset:  

i) How many observations are in your dataset?  Does this number correspond to the originating dataset? 

ii) What are the variable names in the file? How are they formatted?  

#### Here are some commands that will allow you to answer these questions:

From base R:

+ `class(DEMS)` tells you what class the object DEMS belongs to
+ `names(DEMS)` lists the variable names  
+ `dim(DEMS)` gives the dimensions of DEMS
+ `sapply(DEMS,class)` tells you the class of every variable (column)
+ `typeof(DEMS)` says what is the storage mode or R internal type of this object
+ `View(DEMS)`  to open a browser to look at the entire dataset  

From `tidyverse`:

+ `(DEMSt<-as_tibble(DEMS))`  create a tibble version and look at DEMS in the tibble format
+ `select(DEMSt,facilityno,ecdata,nodata,no2data)` allows you to zoom in on specific variables in the DEMS tibble.

Replace the comment in the chunk below with some of these commands.

```{r data.description1}
#-----------------data.description1------------------
# STUDENTS may want to edit this chunk to look at the data differently
#
# Here is a first pass at commands to use to check your data
class(DEMS) # we expect this to be a data frame
dim(DEMS) # we expect this to have 1275 rows and 17 columns
names(DEMS) # this list should correspond to the list in the data summary document
sapply(DEMS,class) # this gives us details on the type of each variable in the dataset
# Now for a tidyverse option.  We will also save the version in case we want to use tidverse commands
(DEMSt<-as_tibble(DEMS)) # create and print the tibble DEMSt (The parentheses surrounding the command tell R to print the result of the assignment.)
select(DEMSt,facilityno,u_s, ecdata,nodata,no2data) # our application of select zooms in to focus on the key exposure data
```

###2. Get some basic understanding of the data 

Here are some commands:

+ `head(DEMS)` shows the first 6 rows of the dataset
+ `tail(DEMS,1)` shows the last row of the dataset
+ `head(DEMS$facilityno,20)` shows the first 20 rows of a subset of variables
+ `summary(DEMS)` gives a basic summary of each variable
+ `describe(DEMS)` is another useful basic summary of the dataset, from the Hmisc package ("a concise statistical description")
+ `xtabs(~facilityno,data=DEMS)` Note:  faciltyno is a factor, ordered by facility number
+ `xtabs(~facilityid,data=DEMS)` Note:  facilityid is a character variable, ordered by facility letter


```{r data.description2}
#-----------------data.description2-------
# Here is a first pass at commands for students to try to better understand the data
head(DEMS) # first 6 rows and ALL variables (columns)
summary(DEMS) # a basic summary of each variable
describe(DEMS) # a different basic summary of each variable
xtabs(~facilityno,data=DEMS) # tallies of the number of observations by facilityno
```


###3. Verify your data correspond to what you expect 

The specifics of what you evaluate depend upon the context of the problem.  In our case we have published papers we can rely upon to determine whether our data are what we expect.  Here are some questions for the DEMS personal data:  <!--Note:  Need a new line for R Markdown to recognize I'm starting a list-->

i) How many observations are in your dataset overall and by facility?  Do these numbers correspond to the originating datasets?  (You can check against the papers.)

ii) Note that there are missing data in this dataset.  As you proceed, you will want to verify whether they are consistent with your expectations.  Presence of missing data requires additional attention to how you will handle these values in R.
        + Which variables have missing values?
        + Which variable(s) have the most missing data?

```{r data.description3}
#-----------------data.description3------------------
# STUDENTS ADD code to look at the data:
#   Which commands from above will allow you to verify observation numbers and missing value counts?
#   In this dataset it is important to understand summaries within mines.  Which commands from above will facilitate that?  (We will learn more below too.)
```

## III. Basic data description

Once you have a basic understanding of your data and believe they were read in correctly, you can focus on quantities of interest.  We will focus on creating a few basic summaries of `ecdata` in this lab.  Next week you will get more experience with other tools such as transformations and checking the distirbution of variables.

###1. Tables and computing descriptive statistics 


* What do you observe about the distribution?  
* Do the descriptive statistics vary by facility?  

Here are some commands:

+ `min(DEMS$ecdata,na.rm=TRUE)` is the minimum after missing values omitted
+ `mean(DEMS$ecdata,na.rm=TRUE)` is the mean after missing values omitted
+ `sd(DEMS$ecdata,na.rm=TRUE)` is the standard deviation after missing values omitted
+ `summaryFull(DEMS[,c("ecdata","nodata","no2data")])` is a full set of summary statistics from the EnvStats package.  It only works for numeric variables.
+ `fivenum(DEMS$ecdata)` shows the five number summary of ecdata (min, lower hinge (close to the 25th percentile), median, upper hinge, maximum).  Note no need to tell it to remove missing missing values here.  Five number summaries are used in box plots and give you a basic understanding of the distribution of a variable.
+ From the EnvStats package, `geoMean(ecdata,na.rm=TRUE)` gives the geometric mean while `geoSD(ecdata,na.rm=TRUE)` gives the GSD.

To produce summary statistics by facility:  Here are two ideas.  Which do you prefer and why?

i)  A simple table of summary statistics using `tapply` and `cbind`.  Note the extra attention needed to handle missing data for the sample size. (What do you notice about the sample sizes here vs. when you just counted observations?  What is the cause of the difference?)
```{r simple.table, echo=F}
#------simple.table using tapply and cbind ----------
#This uses facilityno, a factor variable
xbar<-tapply(DEMS$ecdata,DEMS$facilityno,mean  ,na.rm=T)
sd  <-tapply(DEMS$ecdata,DEMS$facilityno,sd    ,na.rm=T)
n   <-tapply(DEMS$ecdata[!is.na(DEMS$ecdata)],DEMS$facilityno[!is.na(DEMS$ecdata)],length)
cbind(mean=xbar,SD=sd,N=n)
# Note you can present this table in a slightly nicer format using kable from the knitr package:
kable(cbind(mean=xbar,SD=sd,N=n))
```

ii) Here is a `tidyverse` option using `dplyr`:
```{r table.with.dplyr}
#----table.with.dplyr------
facility_ec <- DEMS %>% group_by(facilityno) %>% 
    dplyr::summarize(N = sum(!is.na(ecdata)),
              Nmiss = sum(is.na(ecdata)),
              mean = mean(ecdata, na.rm = T),
              sd = sd(ecdata, na.rm = T),
              se = sd/sqrt(N)
    )

facility_ec
# And here is the same result printed using kable
kable(facility_ec)
```

Note:  One reason to do computations in multiple ways is to help you make sure you are reporting the correct values and that you completely understand your output.  When multiple approaches to the same summary give the same answer, you can be more confident that your answer is correct.

###2. Plots:  Histograms 

#### Make a histogram of ecdata. Add a density curve to the histogram.    

* `ggplot2` (part of `tidyverse`)  has great tools for plotting.  First we show a basic histogram.  Then we switch to the density scale and overlay a normal density with the same mean and variance as the data.  Finally we  overlay a density plot.  
    a. See chunk comments and the notes following the chunk for option suggestions.    
    b. `ggplot` gives warnings about omitted data and messages about better options to choose.  You can prevent these messages from showing up in your rendered document by using the chunk option `warning=FALSE` and `message=FALSE`.  They will show up in the R Markdown console instead if you set these to `FALSE`.  (Note:  We set these options globally in our setup chunk above so we don't need to repeat them below.)

```{r hist.in.tidyverse, warning=FALSE, message=FALSE}
#------hist.in.tidyverse---------
# plot 1 with histogram only and count on the y axis (the default)
# the default binwidth is 30 and often you will want to change it; see plot 2 for a narrower binwidth.
ggplot(data=DEMS,aes(ecdata,na.rm=T)) +
    geom_histogram(colour="black",fill="white")
# plot 2 with density
# base plot, just the histogram, now named 'p' for ease of re-use:
p <- ggplot(data=DEMS,aes(ecdata,na.rm=T)) +
    geom_histogram(aes(y=..density..),colour="black",fill="white",binwidth=20) 
# overlay a normal density plot; need to create some variables to do this
N <- sum(!is.na(DEMS$ecdata))
x <- seq(0,1000,length.out=N) #divides the range 0-1000 into N equal increments
df <- with(DEMS[!is.na(DEMS$ecdata),], data.frame(x, y = dnorm(x, mean(ecdata), sd(ecdata))))
p +  geom_line(data = df, aes(x = x, y = y), color = "red")
# plot 3 now also overlays a kernel density plot
# The alpha parameter (range 0-1) controls the degree of transparency, while the `fill="red"` ensures the overlaid density is a specific color.  We are using the default bandwidth (bw) here.  
p +  geom_line(data = df, aes(x = x, y = y), color = "red") +
  geom_density(alpha=.2,fill="red")
```

Note:  In contrast to the normal density which overlays a probability distribution of a specific form, kernel density smoothers give you smooth curve that track the data.  The smooth curve will be rougher for smaller bandwidths.

To set the amount of smoothing (i.e. the bandwidth) in the kernel density, use the `bw` option.  Kernels are scaled so that `bw` is the standard deviation of the smoothing kernel. To choose the smoothing kernel, use the `kernel` option, e.g. `kernel="gaussian"` (which is the default).

## IV. Create and work with transformed data

Typically exposure data appear to be log-normally distributed.  This section we transform and plot the transformed data.

### Logarithmic transformations using `dplyr`:

* `mutate( DEMS, ln_ecdata = log(ecdata) )` creates the natural log-transformed variable in the DEMS data frame.    
* `mutate( DEMS, log10_ecdata = log10(ecdata) )` creates the base 10 log-transformed variable in the DEMS data frame. 
    
Note that `mutate` adds the new variables to the end of the dataset.  For more functions you can use with `mutate` to create new variables, see R4DS pp. 56-58.

```{r transform.vars}
#-------transform.vars------
# The following two variables will be added to the DEMS dataframe at the end of the frame:
DEMS<-mutate(DEMS,
    ln_ecdata = log(ecdata)
    )
DEMS<-mutate(DEMS,
    log10_ecdata = log10(ecdata)
    )
```

##V. More plots:  Try out some scatterplots

###1. Overview of `ggplot` basics
Here is the basic format of `ggplot`: 
```
ggplot(data = <DATA> ) +
    <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))
```
where `<DATA>` is your dataset, `aes` is the aesthetic for the mapping or the visual properties of the objects in the plot, such as size, shape, and color of points.  The `mapping` is how you apply the variables in the dataset to the aesthetics.  Finally, the `<GEOM_FUNCTION>` is how the mapping will be presented.  Examples are `geom_point` and `geom_line`.  `ggplot` is very expandable, allowing you to layer aspects into your plots, such as to overlay density plots on top of histograms as we did above.  More generally, the layered grammar of `ggplot` is:
```
ggplot(data = <DATA> ) +
    <GEOM_FUNCTION>(
        mapping = aes(<MAPPINGS>),
        stat = <STAT>,
        position = <POSITION>
    ) +
    <COORDINATE_FUNCTION> +
    <FACET_FUNCTION> +
    <LABEL_FUNCTION>
# For more details see R4DS p. 34 (2017 version)
```
where the added pieces are `<STAT>`: statistics to put on the plot, `<POSITION>` to locate your object (such as `"jitter"`"), `<COORDINATE_FUNCTION>` to specify the coordinate system to plot under (e.g. polar coordinates), `<FACET_FUNCTION>` to allow you to divide the plot into subplots, and `<LABEL_FUNCTION>` in order to add titles, axis labels, etc.

For more details on `ggplot` see R4DS, the [R for Data Science](https://r4ds.had.co.nz/) book by Hadley Wickham and Garrett Grolemund.  In particular, Chapter 1 gives a nice introductory overview and Chapter 22 focuses on the details you need to facilitate good communication.

###2. Now plot the relationship between NO (nodata)  and REC (ecdata). 

`ggplot2` makes this easy:  

```{r scatterplot.in.tidyverse, echo=T, warning=F, message=F}
#------scatterplot.in.tidyverse---------------------
# plot 1: simple way to use ggplot to ask for a scatterplot
qplot(nodata, ecdata, data=DEMS) 
# plot 2 is the same plot, now using standard ggplot notation
ggplot(data=DEMS,aes(nodata,ecdata,na.rm=T)) +
    geom_point() 
# plot 3 adds a best fit line added and no 95% CI on the line (se=FALSE option).  We also add a title and axis labels
ggplot(data=DEMS,aes(nodata,ecdata,na.rm=T)) +
    geom_point() + stat_smooth(method=lm, se=FALSE) +
    labs(title="Scatterplot of the DEMS NO (ppm) vs. REC (ug/m3) data",  x="NO (ppm)",
        y="REC (ug/m3)")
# plot 4 replaces the best fit line with a smooth loess curve added and its 95% CI (the default smoother is gam for large datasets and loess when there are less than 1,000 observations)
ggplot(data=DEMS,aes(nodata,ecdata,na.rm=T)) +
    geom_point() + stat_smooth(method="loess") +
    labs(title="Scatterplot of the DEMS NO (ppm) vs. REC (ug/m3) data",  x="NO (ppm)",
        y="REC (ug/m3)") 
##plot 5 adds different colors and smoothers for underground vs. surface data
ggplot(data=DEMS,aes(nodata,ecdata,na.rm=T)) +
    geom_point(mapping=aes(color=u_snum)) +
    geom_smooth(method="loess",aes(color=u_snum))+
    labs(title="Scatterplot of the DEMS NO (ppm) vs. REC (ug/m3) data\ncolored by where measurements were taken",  x="NO (ppm)",y="REC (ug/m3)", color="Measurement\nlocation")
##plot 6 adds facets to plot 1 + colors for underground vs surface
##Note:  the scales="free" option allows a different scale for each plot.  In this case it makes the individual plots more informative.
##Note2:  the smoother was failing for facilty E for unknown reasons; without scales="free" all the plots had huge ranges
ggplot(data=DEMS,aes(na.rm=T)) +
    geom_point(mapping=aes(nodata,ecdata,color=u_snum)) +
    facet_wrap(~facilityno, nrow=2,scales="free")+
    labs(title="Scatterplot of the DEMS NO (ppm) vs. REC (ug/m3) data \nseparately by facility and colored by where the measurements were taken",  x="NO (ppm)",
        y="REC (ug/m3)", color="Measurement\nlocation")

```



Note:  If you make such scatterplots by facility with data on the log base 10 scale and the axes swapped, you should be able to produce plots that look like the results in Figure 2 of Coble et al (2010).  (We will show you how to transform data in R next week.)

---

# Homework exercises

Note:  Refer to the lab write-up guidelines (*LabReportGuidelines.html*) posted on Canvas for the format and content of your lab report.  You may also be interested in some of the tips on the Canvas Wiki page: *Helpful Hints on Lab Report Write-ups*.   
TODO:  Add hyperlinks

(@) Make table(s) summarizing four exposure variables in this dataset:  Nitrogen oxide, nitrogen dioxide, respirable organic carbon, and respirable elemental carbon (variables:  nodata no2data ocdata ecdata).  Include the arithmetic mean (AM), arithmetic standard deviation (ASD), geometric mean (GM), and geometric standard deviation (GSD).  Show these summaries both overall and at the facility level.  Write a few sentences describing the results in the table.  (Challenge version:  How closely can you replicate Tables 1, 2, and 3?) 

    (a.) Note:  You can’t take the log of 0 so you need to decide how to handle the 0’s in the nitrogen oxide and organic carbon data.  Summary statistics from the reduced dataset, i.e. a dataset created by omitting the 0’s, will be misleading.  A simple alternative is to create a new variable that adds a constant to every observation and use this variable when taking logs.  However, the purpose of your analysis matters since adding a constant is not always the appropriate way to handle this challenge.  If you do add a constant, choose the constant to add thoughtfully.  How big should it be?  Should it be the same for NO and OC?  Make sure to document the change to the variables in your lab write-up.

    (b.) Getting GM and/or GSD in a table in R:  I don't yet know how to get the GM and GSD using `tidyverse`, though it should be possible using a combination of `mutate` and `dplyr`.  You can use the `EnvStats` package with `tapply` and `cbind`.  To get the AM, ASD, GM, GSD, N in order:
    ```{r table.with.GM+GSD}
#----table.with.GM+GSD using tapply and cbind ----------
#This uses facilityno, a factor variable
xbar<-tapply(DEMS$ecdata,DEMS$facilityno,mean  ,na.rm=T)
sd  <-tapply(DEMS$ecdata,DEMS$facilityno,sd    ,na.rm=T)
gm  <-tapply(DEMS$ecdata,DEMS$facilityno,geoMean,na.rm=T)
gsd <-tapply(DEMS$ecdata,DEMS$facilityno,geoSD ,na.rm=T)
n   <-tapply(DEMS$ecdata[!is.na(DEMS$ecdata)],DEMS$facilityno[!is.na(DEMS$ecdata)],length)
cbind(AM=xbar,SD=sd,GM=gm,GSD=gsd,N=n)
# we can make this prettier using rounding and kable:
kable(round(cbind(AM=xbar,SD=sd,GM=gm,GSD=gsd,N=n),2))
```

And repeat this exercise for underground only:  (Allows checking against Coble Table 1, and the numbers are similar but not identical.  Without the partitioning by location, the GSD for mine B is enormous and not that believable.  However, I think this is the huge range of the data and not an error.)

```{r UGtable.with.GM+GSD}
#----UGtable.with.GM+GSD using tapply and cbind ----------
#This uses facilityno, a factor variable
DEMSu<-DEMS[DEMS$u_s=="u",]
xbar<-tapply(DEMSu$ecdata,DEMSu$facilityno,mean  ,na.rm=T)
sd  <-tapply(DEMSu$ecdata,DEMSu$facilityno,sd    ,na.rm=T)
gm  <-tapply(DEMSu$ecdata,DEMSu$facilityno,geoMean,na.rm=T)
gsd <-tapply(DEMSu$ecdata,DEMSu$facilityno,geoSD ,na.rm=T)
n   <-tapply(DEMSu$ecdata[!is.na(DEMSu$ecdata)],DEMSu$facilityno[!is.na(DEMSu$ecdata)],length)
cbind(AM=xbar,SD=sd,GM=gm,GSD=gsd,N=n)
# we can make this prettier using rounding and kable:
kable(round(cbind(AM=xbar,SD=sd,GM=gm,GSD=gsd,N=n),2))
```


(@) Make some figures to show the distribution of the REC data.  (Challenge version: Can you do this separately by facility and location?  Can you replicate Figure 1 of Coble?)  Write a few sentences describing what you see in the figures.

(@) Can you replicate the key information shown in Figure 2 of Coble?  What are your ideas for enhancing the plots? (Support your graphical presentation with some brief discussion of what you see in the plots.)

---

# Appendix 1:  Base R and older package commands for reference

### Tables

The following approaches to making tables are available, though not recommended.  These include a. the `ddply()` function in the `plyr` package, b.  `summarizeBy()` from the `doBy` package, and c. `aggregate()` which is part of base R but more difficult to use. 

a) with `ddply()`:  Out of date so code hidden  

```{r table.with.ddply, echo=F, eval=F}
#------------------table.with.ddply------------------
# Note: this is obsolete b/c replaced with dplyr
# Note:  summarize can be spelled with an "s" or "z"
facility_ec<-ddply(DEMS, .(facilityno), plyr::summarize,
            N=sum(!is.na(ecdata)),
            Nmiss=sum(is.na(ecdata)),
            mean=mean(ecdata, na.rm=T),
            sd=sd(ecdata, na.rm=T),
            se=sd/sqrt(N))
facility_ec
```
            
b) with `summaryBy()` from the `doBy` package:  Out of date so code hidden  
```{r table.with.summaryBy, echo=F, eval=F}
#---------------table.with.summaryBy----------------
# Note: this is obsolete b/c replaced by dplyr
fun <- function(x){ 
    c(m=mean(x,na.rm=T), sd=sd(x,na.rm=T), n=sum(!is.na(x)), Nmiss=sum(is.na(x)))
}
# the se hasn't worked for me yet
summaryBy(ecdata~facilityno,data=DEMS,FUN=fun)
summaryBy(no2data~facilityno,data=DEMS,FUN=fun)
summaryBy(nodata~facilityno,data=DEMS,FUN=fun)
```

c) with `aggregate()`:  
The following gives means of 3 variables.  Note:  by comparing the results from facility H6, we can see that the the missing data were removed separately by each variable.  

```{r table.with.aggregate, echo=TRUE, eval=FALSE}
#----------------table.with.aggregate----------------
## part of the stats package

aggregate(DEMS[c("ecdata","no2data","nodata")],
        list(facility=DEMS$facilityno), mean, na.rm=T)
```


### Base R option for a histogram of ecdata, also with a density curve added to the histogram.    

To get a basic histogram, use `hist`.  For a density plot, use `plot()` with `density()`.  (You need to explicitly remove the missing values to use the density function.)

```{r hist.in.baseR, echo=T}
#----------------hist.in.baseR-----------------

#plot 1: basic histogram
hist(DEMS$ecdata)  #can plot without explicitly excluding the missing data
#plot 2: same plot
hist(DEMS$ecdata[!is.na(DEMS$ecdata)]) #same plot, now explicitly removing missing data
#plot 3:  density plot
plot(density(DEMS$ecdata[!is.na(DEMS$ecdata)])) #need to explicitly remove the missing data
```

### Logarithmic transformations in base R:

* `DEMS$ln_ecdata <- log(DEMS$ecdata)` creates the natural log-transformed variable in the DEMS data frame.    
* `DEMS$log10_ecdata <- log10(DEMS$ecdata)` creates the base 10 log-transformed variable in the DEMS data frame.  


### Base R option for a basic scatterplot and overlaid regression line: 

```{r scatterplot.in.baseR}
#----------------scatterplot.in.baseR-----------------
#plot 1 with best fit regression line
plot(DEMS$ecdata,DEMS$nodata)  #gives a basic scatterplot
abline(lm(DEMS$nodata~DEMS$ecdata)) #add a best fit regression line
#plot 2 with scatterplot smoother
scatter.smooth(DEMS$ecdata,DEMS$nodata) #repeat the plot, this time with a loess smooth curve with default span
```


# Appendix 2:  Code and sessioninfo

The next two chunks should be included in every R Markdown appendix so that you document your code and session information.  This supports the reproducibility of your work.  

```{r session.info}
#-----------------session.info: beginning of Appendix -----------------
#This allows reproducibility by documenting the version of R and every package you used.  It also prints your working directory (helpful for finding the project later!).
sessionInfo()
getwd()
```

```{r appendix.code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=T}

```

