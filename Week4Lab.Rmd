---
title: 'Week 4 Lab:  Regression for Prediction'
author: "Lianne Sheppard for ENVH 556"
date: "1/17/2019; Updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        fig_caption: yes
        toc: true
        toc_depth: 3
        number_sections: true
---

<!--Basic document set-up goes here  -->
```{r setup, include=FALSE}
#-------------r.setup-------------
knitr::opts_chunk$set(echo = TRUE)
```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}
#----------------load.libraries.pacman----
# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  Some reasons for packages:
# knitr:  kable()
# ggplot2: part of tidyverse
# readr: part of tidyverse
# dplyr: part of tidyverse
# multcomp:  glht
# modelr:  part of tidyverse and need for add_predictions and add_residuals
# boot:  cv tools are available
# Hmisc:  describe
pacman::p_load(tidyverse, knitr, mutlcomp, dplyr, modelr, Hmisc)  
```

```{r read.data, echo=FALSE}
#-----------read.data-----------------
#-getwd
    ProjectPath <- getwd()
#-dir.create    
    dir.create(file.path(ProjectPath,"Datasets"),     showWarnings=FALSE, recursive = TRUE)
    datapath<-file.path(ProjectPath,"Datasets")
#-read.data
snapshot<-readRDS(file.path(datapath,"allseasonsR.rds"))

```

TODO:  Finish lecture slides
TODO commands for AIC and BIC
TODO:  ask Sun about stepwise code and strategies.  Her parsimonious model should have covered this
TODO:  fix the do_CV function
TODO:  Add code to randomly select groups


# Purpose

The purpose of this lab is to use principles of “out-of-sample” assessment to validate regression models.   We will use the snapshot data for model validation, run a cross-validation, and write a program to more easily repeat cross-validation procedures. You will use these tools to try to understand the bias-variance trade-off in these data.  


# Getting Started

This section gives some basic R commands for regression, prediction, and model validation.  We will also learn how to write loops and programs. 

Steps/ideas to cover:
** Regression reminder + getting predictions
** Regression-based R2
* Forward stepwise regression
* Manual cross-validation code
* Writing functions (general ideas & principles)
* CV function
* Other CV tools we could consider

## Set-up

* Restrict data to one season:  (fall here) 
```{r fall subset}
# fall subset ----
# Traditional R approach
#fall <- subset(snapshot, season==2)
# Tidyverse approach
fall <- filter(snapshot, season == 2)

```


## Commands for regression, plotting, and computing prediction R2:  (see also Week 3 lab)

Regression:  

```{r fall regression}
# fall regression, common model

summary( lm_fall <-
    lm(ln_nox ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm,
       data = fall))

```

Basic predictions, and plotting them:

```{r predictions with dplyr, fig.width=6, fig.height=5}
# predictions with dplyr -----------
snap2 <- snapshot %>%  
    add_predictions(lm_fall,"preds_fall")  

# Here's a plot that shows that the data are for all rows. (We can alse use
# decribe from the Hmisc package.)
ggplot(data = snap2, aes(preds_fall, ln_nox)) +
    geom_point() +
    coord_fixed() +
    facet_wrap( ~ seasonfac) +
    geom_abline(intercept = 0, slope = 1, color = "blue") +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(title = "Fall model predictions vs. ln(NOx) \nby season", 
         x = "Predicted ln(NOx) (ln(ppb))",
         y = "Observed ln(NOx) (ln(ppb))",
         caption = "Best fit line is red; 1:1 line is blue")

```

Correlations of predictions vs. observations by season:

```{r season-specific correlations, eval = FALSE}
# season-specific-correlations ------
# TODO:  figure out why select isn't working
pred_summary <- select(snap2,  c("ln_nox","preds_fall","ID","season")) %>%
    group_by(season) %>%
    summarize(
        r = cor(ln_nox,preds_fall),
        R2 = r^2)
kable(pred_summary, digits = 2)
    
```

## Stepwise regression (forward selection)

TODO:  Modify after input from Sun
TODO:  remove extraneous stuff from this section

The R function `step()` allows stepwise regression.  It has forward, backward, or stepwise regression search algorithms. Forward selection is a useful tool for ordering a sequence of models based on increasing complexity.  

The function `addterm()` from the `MASS` package allows the addition of the next best term to a model.

TODO:  We still need to learn how to relax the entry criteria so we can better use this tool to help us show the bias-variance trade-off.

+ **Stepwise Model Set-up**
First define the smallest starting model (e.g., *null* model) and the largest possible model we will consider (e.g., *full* model). These define the scope.  The function `step()` defines the direction of the model selection (we will only use forward selection), and the number of models fit (the default number of steps is 1000).  `step()` minimizes the AIC.  (This function is available in both base R and in the `MASS` package.)

An alternative is `addterm()` in the `MASS` package.  This allows use of the F-test or a Chi-square test as alternatives.

TODO:  Need to figure out how to modify the statistical significance criterion for these tests.

```{r stepwise set-up for fall snapshot}
# stepwise set-up for fall snapshot ------
null <- lm(ln_nox ~ 1, data=fall)
null
    
#TODO: figure out how to (simply) expand this to more/all of the predictor variables in the dataset
#TODO:  ask Sun about her stepwise code
full <- lm(ln_nox ~ Pop_5000 + D2Comm + D2A1 + D2Ryard + A1_10k + A23_15k + Pop_15000 + Pop_10000 + A1_750 + A23_50 + A23_10k + A1_3k + D2A3 + D2C + A1_300 + A23_5k + Int_300 + D2Rroad + D2Mport + A1_15k + A1_400 + Int_500 + Int_1000 + A23_1k + Pop_3000 + Pop_2500 + D2Lport + Int_3000, data=snapshot.fall)
full
```   


+ **Forward Selection**

The first model listed (the "object"), is the initial model of the search.  Here the scope gives the entire range of models.  It should work by only adding the full model since the null model is already given.  Still need to look up how to control how long this runs for before stopping.

Note: not showing results because the output is very long.
```{r forward selection on fall snapshot, echo=TRUE, eval=FALSE}
step(null, scope=list(lower=null, upper=full), direction="forward")
```


## TODO:  fit in ISLR validation approach where belongs or DROP

ISLR book validation approach code:
```{r}

# need to adjust dataset, etc.  Gives out of sample predictions...
train=sample(392,196)
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
```


## Manual cross-validation 

```{r a manual CV}
# manual CV ------

#create a variable to store CV predictions 
CV.pred <- NULL

# loop over the 10 clusters
for (i in 1:10){
    # define the current cluster variable as a logical vector
    is.cluster <- fall$cluster == i
    
    # fit the linear model, omitting cluster i (training set)
    CV.lm <-
        lm(ln_nox ~ D2A1 + A1_50 + A23_400 +               Pop_5000 + D2C + Int_3000 + D2Comm,
        data = fall[-is.cluster, ])
  
    # save the predictions and observations in CV.pred for the current cluster (test set)
    CV.results <- fall %>%
        add_predictions(CV.lm,"preds") %>%
        filter(cluster == i) %>%
        select(ID, cluster, ln_nox, preds)
        
    CV.pred <- rbind(CV.pred, CV.results)
}

# now get the MSE, RMSE, MSE-based R2
# mean of observations
ln_nox_avg <- mean(CV.pred$ln_nox)
# MSE of predictions
MSE.pred <- mean((CV.pred$ln_nox - CV.pred$preds)^2)
# MSE of observations (for R2 denominator)
MSE.obs <- mean((CV.pred$ln_nox - ln_nox_avg)^2)
# print the results
paste("RMSE:  ", sqrt(MSE.pred))
paste("MSE-based R2:  ", 
      max(1 - MSE.pred/MSE.obs, 0))
     
```


## Functions for cross-validation and code for MSE 

### Tips on writing functions

TODO:  Add link to R4DS ch 15
From R4DS chapter on functions, Chapter 15, the reason to write functions is to automate repetitive tasks and avoid copying and pasting.  This eliminates the need to update the code in multiple places when you make a change and reduces your chances of making mistakes.  A simple rule of thumb is to write a function whenever you have or will need at least three copies of a block of code.

Here are the 3 steps to writing a function:

1. Pick a name for the function.
2.  List the inputs or *arguments* to the function inside the function call.
3.  Put code to accomplish what you want in the body of the function.

First we write a function to estimate the MSE.  We call it `getMSE`.  Here it is:

```{r define getMSE}
# define getMSE function -----
# Function to get the MSE, RMSE, MSE-based R2
getMSE <- function(obs,pred) {
    # mean of obs
    obs_avg <- mean(obs)
    # MSE of obs (for R2 denominator)
    MSE.obs <- mean((obs-obs_avg)^2)
    # MSE of predictions
    MSE.pred <- mean((obs - pred)^2)
    result <- c(sqrt(MSE.pred),
    max(1 - MSE.pred/MSE.obs, 0))
    names(result) <-  c("RMSE", "R2(MSE-based)")
    result
}

```

Now test our function, using e.g. the manual CV we completed:
```{r test getMSE}
# test getMSE -----
getMSE(CV.pred$ln_nox,CV.pred$preds)

```

Now convert our cross-validation to a function:
TODO:  Get this to work and decide how fancy to allow it to be, e.g. w.r.t. what gets passed to lm().

```{r CV function}
# CV function -------
do_CV <- function(obs, formula, group, dataset){
    # TODO: should be able to extract obs from the formula
    # obs is the outcome
    # formula to pass to lm
    # group is the grouping variable
    # dataset is the data
    # the function returns the dataset with a new variable called cvpreds, out-of-sample predictions   

    # create a variable to store CV predictions 
    CV.pred <- NULL
    
    # get the number of distict clusters 
    k <- length(unique(group))
    
    # loop over all the clusters
    for (i in 1:k){
        # define the current group variable as a logical vector
        is.group <- group == i
    
        # fit the linear model, omitting cluster i (training set)
        CV.lm <-
            lm(formula, data=dataset,
            subset = -is.cluster)
  
        # save the predictions and observations in CV.pred for the current group (test set)
         CV.results <- dataset %>%
            add_predictions(CV.lm,"cvpreds") %>%
            filter(group == i) 
        
        CV.pred <- rbind(CV.pred, CV.results)
    }
# return the dataset; note it will be ordered by group now
CV.pred
}
```


## Bias-variance trade-off analysis 


### Repeated cross-validation for the bias-variance trade-off plot 

Assignment Request:	Use the order of entry into the stepwise to sequentially complete the following computations.  For models with one up to the maximum number of selected variables:

        i.  Use the full dataset to obtain in-sample estimates of the RMSE and R2.
        
        ii. Estimate predicted values using cross-validation.
        
        iii.  Compute out-of-sample RMSE and MSE-based R2 estimates.
 
TODO:  UPDATE       
Code below was kindly provided by David Scoville: (basic tests work; need to integrate)
```{r Model selection and CV, echo=TRUE}

#empty<-lm(ln_nox ~ 1, data=snapshot.fall)

#full<-Pop_5000 + D2Comm + D2A1 + D2Ryard + A1_10k + A23_15k + Pop_15000 + Pop_10000 + A1_750 + A23_50 + A23_10k + A1_3k + D2A3 + D2C + A1_300 + A23_5k + Int_300 + D2Rroad + D2Mport + A1_15k + A1_400 + Int_500 + Int_1000 + A23_1k + Pop_3000 + Pop_2500 + D2Lport + Int_3000,data=snapshot.fall)

#addterm(empty, scope=full, test="F",trace=T)

full2<-c("Pop_5000" , "D2Comm" , "D2A1" , "D2Ryard" , "A1_10k" , "A23_15k" , "Pop_15000" , "Pop_10000" , "A1_750" , "A23_50" , "A23_10k" , "A1_3k" , "D2A3" , "D2C" , "A1_300" , "A23_5k" , "Int_300" , "D2Rroad" , "D2Mport" , "A1_15k" , "A1_400" , "Int_500" , "Int_1000" , "A23_1k" , "Pop_3000" , "Pop_2500" , "D2Lport", "Int_3000")

all.models.CV.perf<-vector("list",length=28)
all.models.in.perf<-vector("list",length=28)

model.terms<-"ln_nox"
for (i in 1:length(full2)) {

  #keep updating model.terms for each added model
model.terms<-c(model.terms,full2[i])
  #defines the variables to be fit for this iteration i
model.vars<-snapshot.fall[,model.terms]
  #created a dataset that also has the cluster variable included
model.vars.cluster<-cbind(snapshot.fall$cluster,model.vars)
  #name the cluster column
colnames(model.vars.cluster)[1]<-"cluster"
  #fit this model on the entire dataset
new.mod<-lm(ln_nox ~ .,data=model.vars.cluster[,2:dim(model.vars.cluster)[2]])
#print(new.mod)
  #extract the MSE from the results:
  # not sure what the following does; substitute the commands after to get the in-sample MSE, etc in the expected format
    #new.mod.perf<-c(mse(new.mod)[2,])
MSE.newmod<-mean(new.mod$residuals^2)
RMSE.newmod<-sqrt(MSE.newmod)
R2.newmod<-summary(new.mod)$r.squared

all.models.in.perf[[i]]<-c(MSE.newmod,RMSE.newmod,R2.newmod)
names(all.models.in.perf[[i]])<-c("MSE.newmod","RMSE.newmod","R2.newmod")

CV.pred.store.snap<-NULL
    for (j in 1:10) {
      model.vars.cluster.reg<-subset(model.vars.cluster,cluster!=j)
      model.vars.cluster.pred<-subset(model.vars.cluster,cluster==j)
      
      
          CV.lm<-lm(ln_nox ~ .,data=model.vars.cluster.reg[,2:dim(model.vars.cluster.reg)[2]])
            
            #print(summary(CV.lm))
          
          CV.predict<-predict(CV.lm,model.vars.cluster.pred[,2:dim(model.vars.cluster.pred)[2]],
            interval="prediction",
            level=0.95,type="response")
          
          CV.pred.store.snap<-rbind(CV.pred.store.snap,cbind(
            snapshot.fall[which(snapshot.fall$cluster==j),c("cluster","ln_nox")],CV.predict))
          
          colnames(CV.pred.store.snap)[c(1,2)]<-c("cluster","ln_nox")
      
    }
  
MSE.pred.CV<-mean((CV.pred.store.snap[,"ln_nox"]-CV.pred.store.snap[,"fit"])^2)
MSE.pred.CV
RMSE.pred.CV<-sqrt(MSE.pred.CV)
#*MSE-based R2
#gen MSER2 = max((1 - MSEests/MSEln_nox),0)
MSER2.CV<-max((1-MSE.pred.CV/MSE.ln_nox),0)
#*the following shows the calculated variables 
#*    (as one line of Stata code)
#display MSEln_nox ",  MSE = " MSEests ",  MSE-based R2 =  " MSER2 
#display "RMSE =  " sqrt(MSEests)
#switched order in last two from what David did
CV.results.subset<-cbind(MSE.ln_nox,MSE.pred.CV,RMSE.pred.CV,MSER2.CV)

#CV.results.subset

all.models.CV.perf[[i]]<-CV.results.subset
#added names, in new order
#names(all.models.CV.perf[[i]])<-c("MSE.ln_nox","MSE.pred.CV","RMSE.pred.CV","MSER2.CV")

}


errors.CV<-matrix(unlist(all.models.CV.perf),ncol=4,byrow=T)

errors.in<-matrix(unlist(all.models.in.perf),ncol=3,byrow=T)
#7x28 matrix, I think
errors<-cbind(errors.CV,errors.in)
errors<-as.data.frame(errors)
colnames(errors)<-c(colnames(all.models.CV.perf[[1]]),names(all.models.in.perf[[1]]))
errors$mod.size<-row.names(errors)

errors.CV.plot<-errors.CV[,3:4]
colnames(errors.CV.plot)<-c("RMSE","R2")
errors.in.plot<-errors.in[,c(2,3)]
colnames(errors.in.plot)<-c("RMSE","R2")


errors.plot.R2<-c(errors.CV.plot[,"R2"],errors.in.plot[,"R2"])
errors.plot.R2<-as.data.frame(errors.plot.R2)
errors.plot.R2$mod.size<-rep(row.names(errors),2)
errors.plot.R2$type<-c(rep("MSE based (CV)",28),rep("Regression Based (in sample)",28))
colnames(errors.plot.R2)[1]<-"R2"

# plots with both model based (in sample) R2 and CV generated MSER2

plot.R2<-ggplot(errors.plot.R2,aes(x=as.numeric(mod.size),y=R2,color=type)) + geom_point() + geom_smooth(se=F) +
 xlab("Model Complexity (# of terms)") + ylab(bquote(bold(R^2)))+
scale_x_continuous(breaks=c(seq(0,30,5))) + theme_bw() + theme(axis.text=element_text(face="bold"),axis.title=element_text(face="bold")) + scale_color_discrete(name="Source",guide = guide_legend(reverse=TRUE))

print(plot.R2)

errors.plot.RMSE<-c(errors.CV.plot[,"RMSE"],errors.in.plot[,"RMSE"])
errors.plot.RMSE<-as.data.frame(errors.plot.RMSE)
errors.plot.RMSE$mod.size<-rep(row.names(errors),2)
errors.plot.RMSE$type<-c(rep("Cross Validation",28),rep("In Sample",28))
colnames(errors.plot.RMSE)[1]<-"RMSE"

# plots with RMSE for both insample and CV

plot.RMSE<-ggplot(errors.plot.RMSE,aes(x=as.numeric(mod.size),y=RMSE,color=type)) + geom_point() + geom_smooth(se=F) +
 xlab("Model Complexity (# of terms)") + ylab("RMSE")+
scale_x_continuous(breaks=c(seq(0,30,5))) + theme_bw() + theme(axis.text=element_text(face="bold"),axis.title=element_text(face="bold")) + scale_color_discrete(name="Source")

print(plot.RMSE)


#plot of just CV based MSE based R2
bvt.MSER2<-ggplot(errors,aes(x=as.numeric(mod.size),y=MSER2.CV)) + geom_point() + geom_smooth(se=F) +
 xlab("Model Complexity (# of terms)") + ylab(bquote(bold(R^2)))+
scale_x_continuous(breaks=c(seq(0,30,5))) + theme_bw() + theme(axis.text=element_text(face="bold"),axis.title=element_text(face="bold")) 

#bvt.MSER2

#plot of just CV RMSE
bvt.RMSE<-ggplot(errors,aes(x=as.numeric(mod.size),y=RMSE.pred.CV)) + geom_point() +geom_smooth(se=F) +xlab("Model Complexity (# of terms)") + ylab("RMSE") +scale_x_continuous(breaks=c(seq(0,30,5))) +theme_bw() + theme(axis.text=element_text(face="bold"),axis.title=element_text(face="bold")) 

#bvt.RMSE




```

 
## Bias-variance trade-off plot 

TODO:  Update.  See example above

# Practice Session

TODO:  Convert from Stata to R

This section covers basic practice to be completed during the lab.   We are going to use the snapshot data described in Mercer et al 2011, discussed in class, and used last week.  It can be found on the class website.  Note that in this lab we are treating the errors are independent and identically distributed even though this assumption is not correct in these data.  (This is the same assumption typically made in LUR modeling.)

Perform the following tasks:
1.	Determine the R project you will use.
2.	Explore the dataset a bit, focusing on one particular season.  Make sure you have some basic understanding of the outcome variable (ln_nox), the CV grouping variable (cluster), and the large number of covariates you can choose from.   In this lab you should restrict your analysis to one season (e.g. winter).  
3.	Fit the model for one of the seasons given in Table 4 of Mercer et al. Make note of these in-sample estimates of R2 and RMSE.
4.	Try to manually cross-validate this model using the code given above.  Compare the CV R2 and RMSE to your in-sample estimates.  (Note:  Use the cluster variable in the dataset to define your CV groups.)
5.	Use the cross-validation function and repeat your cross-validation analysis.  (If you use the same groups, you should get the same results as in the previous step.)
6.	Make a scatterplot comparing ln_nox (the observed dependent variable) on the x-axis with the cross-validated predictions on the y-axis.  Add the 1:1 line to your plot. (If you also want to show the best-fit line, you’ll need to put the predictions on the x-axis rather than the y-axis.)
7.	Create your own version of the bias-variance trade-off plot shown in class using the following steps:
a.	Do a forward stepwise regression of ln_nox on a set of plausible variables with a lax entry criterion (e.g. ADD).   (You may restrict your attention to the list in the forward selection model example given above.)  Keep track of the order the variables were added.  
b.	Use the order of entry into the stepwise to sequentially complete the following computations.  For models with one up to the maximum number of selected variables:
i.	Use the full dataset to obtain in-sample estimates of the RMSE and R2.
ii.	Estimate predicted values using cross-validation.
iii.	Compute out-of-sample RMSE and MSE-based R2 estimates.
c.	In a table or figure(s), summarize the number of variables in each model along with the R2 and/or RMSE estimates from CV and the training data.   If you choose to show your results in a plot, put the number of variables in the model on the x-axis vs. the R2 or RMSE estimates on the y-axis.  Distinguish the two kinds of estimates on your plot.  If you choose to show your results in a table, also include a column for the variable name of the variable added.



# Homework Exercises

1.	Write a brief summary of the purpose of the lab and your approach.  Then present your results:

    a.  Describe the results (with appropriate displays in table(s) and/or figures(s)), and
    
    b.  Discuss the insights you have obtained from your analyses, both of the training data alone and after cross-validation. In your discussion, comment on how your in-sample and cross-validated MSE-based R2 estimates compare.
    
2.	**Extra credit**. Present one or both of the following results in your write-up:

    a.  Repeat the exercise using randomly defined CV groups that ignore the gradient clusters.
    
    b.  Repeat the exercise using yet another different set of CV groups, either chosen randomly, or spatially, or based on some other criterion. 



# Other resources to explore

TODO:  Update and edit.  Load appropriate packages

## Cross Validation using the R packages (cvTools and/or caret)
NOTE:  This has not been tested yet.  We need to determine whether this package gives the same results as our manual procedures, or not.  Also, the worked example below is randomly choosing CV groups, which is a very different approach from using the cluster variable.

Of the three commonly used cross-validation methods (random subsampling, k-fold cross-validation and leave-one-out cross-validation), here we will focus on k-fold cross-validation. For more details on cross-validation, please refer to the **Lecture Notes** and <https://www.r-bloggers.com/cross-validation-for-predictive-analytics-using-r/>.

The following uses tools and examples from *cvTools* package to write a program to cross-validate results from our linear model.

Set seed for reproducibility
```{r echo=TRUE}
set.seed(1234)
```

Set-up folds for cross-validation, using random allocation into groups.  (Recall the snapshot data has clusters already assigned that keep road gradient sites together.)
```{r set up CV folds, echo=TRUE,eval=FALSE}
folds <- cvFolds(nrow(fall), K = 10, R = 10)
```

Perform cross-validation for a least square regression model 
```{r perform CV, echo=TRUE,eval=FALSE}
fit_LM <- lm(ln_nox ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm, data = fall)
cv_results <- repCV(fit_LM, cost = rtmspe, folds = folds, trim = 0.1)
```

Summarize and plot results of cross-validation
```{r summarize CV, echo=TRUE, eval=FALSE}
summary(cv_results)
bwplot(cv_results)

plot(cv_results, method = "density")
```




### Cross-validation code; not in a function:  TODO: decide if we should drop this 

The `caret` package has code for k-fold cross-validation, as does the `boot` package.  and the `cvTools` package.  Not sure which to prioritize.

#### CV code from stackoverflow; uses a nnet

```{r}
set.seed(450)
cv.error <- NULL
k <- 10

library(plyr) 
pbar <- create_progress_bar('text')
pbar$init(k)

## Assign samples to K folds initially
index <- sample(letters[seq_len(k)], nrow(data), replace=TRUE)
for(i in seq_len(k)) {
    ## Make all samples assigned current letter the test set
    test_ind <- index == letters[[k]]
    test.cv <- scaled[test_ind, ]
    ## All other samples are assigned to the training set
    train.cv <- scaled[!test_ind, ]

    ## It is bad practice to use T instead of TRUE, 
    ## since T is not a reserved variable, and can be overwritten
    nn <- neuralnet(f,data=train.cv,hidden=c(5,2),linear.output=TRUE)

    pr.nn <- compute(nn,test.cv[,1:13])
    pr.nn <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)

    test.cv.r <- (test.cv$medv) * (max(data$medv) - min(data$medv)) + min(data$medv)

    cv.error[i] <- sum((test.cv.r - pr.nn) ^ 2) / nrow(test.cv)
       pbar$step()
}
    
```


# Appendix

```{r session.info}
#-----------------session.info: beginning of Appendix -----------------
#This allows reproducibility by documenting the version of R and every package you used.
sessionInfo()
```

```{r appendix.code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=T}

```


