---
title: 'Week 4 Lab:  Regression for Prediction'
author: "Lianne Sheppard for ENVH 556"
date: "Winter 2019; Updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        fig_caption: yes
        toc: true
        toc_depth: 3
        number_sections: true
---

<!--Basic document set-up goes here  -->
```{r setup, include=FALSE}
#-------------r.setup-------------
knitr::opts_chunk$set(echo = TRUE)
```

TODO:  My interactive running of the following isn't working.  Error says "Error in readRDS(dest) : error reading from connection"  Probably because I'm not connected to the internet and at least one package isn't installed locally.

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}
#----------------load.libraries.pacman----
# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  Some reasons for packages:
# knitr:  kable()
# ggplot2: part of tidyverse
# readr: part of tidyverse
# dplyr: part of tidyverse
# multcomp:  glht
# modelr:  part of tidyverse and need for add_predictions and add_residuals
# boot:  cv tools are available
# Hmisc:  describe
pacman::p_load(tidyverse, knitr, mutlcomp, dplyr, modelr, Hmisc)  
```

```{r read.data, echo=FALSE}
#-----------read.data-----------------
#-getwd
    ProjectPath <- getwd()
#-dir.create    
    dir.create(file.path(ProjectPath,"Datasets"),     showWarnings=FALSE, recursive = TRUE)
    datapath<-file.path(ProjectPath,"Datasets")
#-read.data
snapshot<-readRDS(file.path(datapath,"allseasonsR.rds"))

```

TODO:  Finish lecture slides
TODO:  Add code to randomly select groups
TODO:  Work on the automating of the bias-variance plot
TODO:  Learn about and try out kableExtra for tables (see Nancy's week 1 lab for an example)
TODO:  install caret and cvTools for other cv approaches


# Purpose

The purpose of this lab is to use principles of “out-of-sample” assessment to validate regression models.   We will use the snapshot data for model validation, run a cross-validation, and write a program to more easily repeat cross-validation procedures. You will use these tools to try to understand the bias-variance trade-off in these data.  


# Getting Started

This section gives some basic R commands for regression, prediction, and model validation.  We will also learn how to write loops and programs. 

Steps/ideas to cover:
** Regression reminder + getting predictions
** Regression-based R2
* Forward stepwise regression
* Manual cross-validation code
* Writing functions (general ideas & principles)
* CV function
* Bias-variance plot
* Other CV tools we could consider

## Set-up

* Restrict data to one season:  (fall here) 
```{r fall subset}
# fall subset ----
# Traditional R approach
#fall <- subset(snapshot, season==2)

# Tidyverse approach
fall <- filter(snapshot, season == 2)

```

* Common model names, for later use
```{r}
# common model names -------
covars_common <- c("D2A1", "A1_50", "A23_400",  "Pop_5000", "D2C", "Int_3000", "D2Comm")
# facier way to do this, showing how to make paste0, grep, and/or grepl useful tool:
# TODO:  code this
#covars_common <- c(D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm)
```

## Commands for regression, producing AIC, plotting, and computing prediction R^2^

See also Week 3 lab for these tools and variations of applying them.

* Regression:  

```{r fall regression}
# fall regression, common model-----

summary( lm_fall <-
    lm(ln_nox ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm,
       data = fall))

```

* AIC and BIC:  Note, you can provide these functions with multiple model objects to facilitate comparisons of AIC or BIC across models fit on the same data.

```{r AIC and BIC}
# AIC and BIC -------
# AIC uses a default penalty of 2
AIC(lm_fall)
# BIC uses a penalty of log(n) where n is the number of observations in the dataset
BIC(lm_fall)
```

* Extract predictions, and plot them:  (Note: the `fig.width` and `fig.height` options to a chunk give local definitions for the figure height and width.)

```{r predictions with modelr, fig.width=6, fig.height=5}
# predictions with model -----------
snap2 <- snapshot %>%  
    modelr::add_predictions(lm_fall,"preds_fall")  

# Here's a plot that shows that the data are for all rows. (We can alse use
# decribe from the Hmisc package.)
ggplot(data = snap2, aes(preds_fall, ln_nox)) +
    geom_point() +
    coord_fixed() +
    facet_wrap( ~ seasonfac) +
    geom_abline(intercept = 0, slope = 1, color = "blue") +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(title = "Fall model predictions vs. ln(NOx) \nby season", 
         x = "Predicted ln(NOx) (ln(ppb))",
         y = "Observed ln(NOx) (ln(ppb))",
         caption = "Best fit line is red; 1:1 line is blue")

```

*Correlations of predictions vs. observations by season:  (Note:  these are regression-based R^2^ estimates.)

```{r season-specific correlations, eval = FALSE}
# season-specific-correlations ------
pred_summary <- dplyr::select(snap2,  c("ln_nox","preds_fall","ID","season","seasonfac")) %>%
    group_by(seasonfac) %>%
    summarize(
        r = cor(ln_nox,preds_fall),
        R2 = r^2)
#TODO: Consider replacing with kableExtra
kable(pred_summary, digits = 2)
    
```

## Commands for stepwise regression (forward selection)

The R function `step()` allows stepwise regression.  It has forward, backward, or stepwise regression search algorithms. Forward selection is a useful tool for ordering a sequence of models based on increasing complexity.  

Note:  The `step` function is available in both base R and in the `MASS` package.  While we won't use it in this lab, the function `addterm()` from the `MASS` package allows the addition of the next best term to a model.  This allows use of the F-test or a Chi-square test as alternatives. 

+ **Model Set-up**
First define the smallest starting model (the *null* model) and the largest possible model we will consider (the *full* model). These define the scope.  The function `step()` has options to specify the direction of the model selection (we will only use forward selection or `direction = "forward"`), the amount of output (we'll use `trace = 0` to omit printing any output) and the number of models fit (the default number of models fit, or `steps`, is 1000).  `step()` minimizes the AIC and the option `k` controls the degree of penalty.  Since we don't want to stop early, we use `k = 0` which seems to not impose a stopping criterion.  More research is needed to understand exactly how the `k` option operates w.r.t. stopping the alorithm.


```{r forward selection set-up for fall snapshot}
# forward selection set-up for fall snapshot ------
# define the smallest model of interest, an intercept only model here
null <- lm(ln_nox ~ 1, data=fall)
#null
    
# create the larges possible model, a full model that includes all of the predictor variables in the dataset.  Steps:
# 1: get the list of all the potential covariates of interest from the dataset:
covars_all <- names(fall[12:74])
# 2: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_nox ~ ", paste(covars_all, collapse= "+")))
#full

```   

+ **Forward Selection**

The first model listed in the command (the "object"), is the initial model used to start the search.  In forward regression we want to start with the smallest model of interest to us, here the intercept only model.  The scope gives the entire range of models by also incorporating the largest possible model.  

```{r fitting forward selection}
# forward selection model------
# Note:  k=0 appears to put no restriction on the forward selection and doesn't
# stop until the full model is incorporated. Using k=2 is comparable to AIC.
# Using log(n), where n is the number of observations, is comparable to BIC.
forwardreg_fall <- step(null, scope=list(lower=null, upper=full), trace = 0, direction="forward", k=0)

# save the ordered list of names for later use, dropping the intercept
covars_forward <- names(forwardreg_fall$coefficients)[-1]

```

```{r explore stepwise results, eval=FALSE}
# explore results from forward stepwise-----
# structure of the object
str(forwardreg_fall)
# class of the object
class(forwardreg_fall)
# gives the names of the coefficients for use in other work.  Should assign a name to it for later use.
names(forwardreg_fall$coefficients)
cat("AIC:  ", "\n")
forwardreg_fall$anova$AIC
cat("Deviance:  ", "\n")
forwardreg_fall$anova$Deviance
forwardreg_fall$anova$Step
#forwardreg_fall$anova$Df
```


## Manual cross-validation and summarization

TODO:  expand ISLR and link to it
Note:  We are summarzing the cross-validation results overall.  This is one valid way to do the summary.  Other approaches summarize the results in each cluster (or cross-validation group) separately and average these.  (See e.g. ISLR textbook.)

```{r a manual CV}
# manual CV ------

#create a variable to store CV predictions 
CV.pred <- NULL

# loop over the 10 clusters
for (i in 1:10){
    # define the current cluster variable as a logical vector
    is.cluster <- fall$cluster == i
    
    # fit the linear model to the training set by omitting cluster i 
    CV.lm <-
        lm(ln_nox ~ D2A1 + A1_50 + A23_400 +               Pop_5000 + D2C + Int_3000 + D2Comm,
        data = fall[-is.cluster, ])
  
    # save the predictions and observations in CV.pred for the current cluster (test set)
    CV.results <- fall %>%
        modelr::add_predictions(CV.lm,"preds") %>%
        filter(cluster == i) %>%
        dplyr::select(ID, cluster, ln_nox, preds)
    # bind the results together to get back the full dataset    
    CV.pred <- rbind(CV.pred, CV.results)
}

# now get the MSE, RMSE, MSE-based R2
# mean of observations
ln_nox_avg <- mean(CV.pred$ln_nox)
# MSE of predictions
MSE.pred <- mean((CV.pred$ln_nox - CV.pred$preds)^2)
# MSE of observations (for R2 denominator)
MSE.obs <- mean((CV.pred$ln_nox - ln_nox_avg)^2)
# print the results
paste("RMSE:  ", round( sqrt(MSE.pred), 2))
paste("MSE-based R2:  ", 
      round( max(1 - MSE.pred/MSE.obs, 0), 2))
     
```


## Functions for cross-validation and MSE 

### Tips on writing functions

TODO:  Add link to R4DS ch 15  
TODO:  Get input from Brian on other recommendations and improvements to the general guidance (and presumably my functions).

From the *R for Data Science* (R4DS) chapter on functions, Chapter 15, the reason to write functions is to automate repetitive tasks and avoid copying and pasting.  This eliminates the need to update the code in multiple places when you make a change and reduces your chances of making mistakes.  A simple rule of thumb is to write a function whenever you have or will need at least three copies of a block of code.

Here are the 3 basic steps to writing a function as described in R4DS:

1. Pick a name for the function.
2.  List the inputs or *arguments* to the function inside the function call.
3.  Put code to accomplish what you want in the body of the function.

To start our learning process, first we write a function to estimate the MSE and MSE-based R^2^.  We call it `getMSE`.  Note that in addition to its name and arguments, there are key formatting details to follow.  In particular, pay attention to the arguments inside the parentheses in the function command, and the use of curly brackets. 

```{r define getMSE}
# define getMSE function -----
# This is a function to get the MSE, RMSE, MSE-based R2
getMSE <- function(obs,pred) {
    # mean of obs
    obs_avg <- mean(obs)
    # MSE of obs (for R2 denominator)
    MSE.obs <- mean((obs-obs_avg)^2)
    # MSE of predictions
    MSE.pred <- mean((obs - pred)^2)
    result <- c(sqrt(MSE.pred),
    max(1 - MSE.pred/MSE.obs, 0))
    names(result) <-  c("RMSE", "R2(MSE-based)")
    result
}

```

Now test our function, using the manual CV results we obtained:
```{r test getMSE}
# test getMSE -----
getMSE(CV.pred$ln_nox,CV.pred$preds)

```

Now convert our cross-validation to a function:

TODO:  Decide how fancy to allow it to be, e.g. w.r.t. what gets passed to lm().  
TODO:  Get suggestions from Brian on how to write this better.

```{r CV function}
# CV function -------
do_CV <- function(obs, formula, group, data){
    # TODO: probably need to also add the group variable to the dataset in case it is not already part of the dataset OR do like the manual CV and only output certain variables.  (However, then we risk not being able to do more with the results if we wish.)
    # TODO: is it OK to use data as an argument here, or is it too confusing??
    # obs is the outcome
    # formula to pass to lm
    # group is the grouping variable
    # data is the data frame
    # the function returns the dataset with a new variable called cvpreds, out-of-sample predictions   

    # create a variable to store CV predictions 
    CV.pred <- NULL
    
    # get the number of distict clusters 
    k <- length(unique(group))
    
    # loop over all the clusters
    for (i in 1:k){
        # define the current group variable as a logical vector
        is.group <- group == i
    
        # fit the linear model to the training set by omitting cluster i 
        CV.lm <-
            lm(formula, data=data,
            subset = -is.cluster)
  
        # save the predictions and observations in CV.pred for the current group (test set)
         CV.results <- data %>%
            modelr::add_predictions(CV.lm,"cvpreds") %>%
            filter(group == i) 
        
        CV.pred <- rbind(CV.pred, CV.results)
    }
# return the dataset; note it will be ordered by group now (TODO:  decide whether this is a good feature or should be changed)
CV.pred
}

```

```{r test do_CV}
# test do_CV------

# first create the formula from a vector of names
common_model_formula <- as.formula(paste("ln_nox ~ ", paste(covars_common, collapse= "+")))

# Note: the do_CV function only works if the group variable is a vector and not part of a dataset.
# TODO: get rid of the above feature if straightforward
# should also append the grouping variable to the output dataset for cases when it isn't part of the input dataset
temp <- do_CV(ln_nox, common_model_formula, group = fall$cluster, data = fall)

```

```{r check CV results}
# check CV results -------
# now check the results from the CV function
# Summarize with getMSE
getMSE(temp$ln_nox, temp$cvpreds)

# now look at the scatterplot, observations on the x axis as we typically do for considering out-of-sample predictions
ggplot(data = temp, aes(ln_nox, cvpreds)) +
    geom_point() +
    coord_fixed() +
    geom_abline(intercept = 0, slope = 1, color = "blue") +
    labs(title = "Fall model predictions vs. ln(NOx) \nCross-validated", 
         x = "Observed ln(NOx) (ln(ppb))",
         y = "Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is blue")

```

## Bias-variance trade-off analysis 

Assignment Request:	Use the order of entry into the stepwise to sequentially complete the following computations.  For models with one up to the maximum number of selected variables:  
        i.  Use the full dataset to obtain in-sample estimates of the RMSE and R2.  
        ii. Estimate predicted values using cross-validation.  
        iii.  Compute out-of-sample RMSE and MSE-based R2 estimates.  
 
Steps to coding this:
* compute/retrieve the vector of names output from the forward selection model
* loop over the maximum number of covariates from forward selection
* create a formula that is the linear combination of the first i terms of the forward selection model
* summarize the in-sample prediction MSE and R2
* cross-validate this and summarize the out-of-sample MSE and R2
* store the key set of results, plus the number of covariates in the model and possibly the variable added to use in plotting
* use ggplot to make the bias-variance trade-off plot

TODO:  UPDATE --- ran out of time to work on this on my plane ride ....      
```{r Model selection and CV, echo=TRUE}
# Model selection and CV
# Credit:  The initial version of this code was kindly provided by David Scoville
# TODO:  NOT yet tested or evaluated

#get the length of the vector of names from forward selection
n_covars <- length(covars_forward)

# set up an object to store the results in 
results_store <- NULL

# loop over n_covars
for (i in 1:n_covars) {
    # formula, updated to add a term each time
    fmla <- as.formula(paste("ln_nox ~ 1 + ", paste(covars_all[1:i], collapse= "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(formula, data = fall) 
    in_MSE <- summary(in_model$sigma)
    in_R2 <- summary(in_model$r.squared)
    
    # out-of sample model and estimates
    out_ests <- do_CV(ln_nox, fmla, fall$cluster, data = fall)
    out_results <- getMSE(out_ests$ln_nox, out_ests$cvpreds)
    out_MSE <- out_results[1]
    out_R2 <- out_results[2]
    
    # store the results in results_store
    results_store <- cbind(results_store, c(i, covars_all[i], in_MSE, in_R2, out_MSE, out_R2))
    }


#START HERE -- add ggplot; much of the following should be redone or dropped


# David's code for ggplot; needs to be revised
# plots with both model based (in sample) R2 and CV generated MSER2

plot.R2<-ggplot(errors.plot.R2,aes(x=as.numeric(mod.size),y=R2,color=type)) + geom_point() + geom_smooth(se=F) +
 xlab("Model Complexity (# of terms)") + ylab(bquote(bold(R^2)))+
scale_x_continuous(breaks=c(seq(0,30,5))) + theme_bw() + theme(axis.text=element_text(face="bold"),axis.title=element_text(face="bold")) + scale_color_discrete(name="Source",guide = guide_legend(reverse=TRUE))

print(plot.R2)

errors.plot.RMSE<-c(errors.CV.plot[,"RMSE"],errors.in.plot[,"RMSE"])
errors.plot.RMSE<-as.data.frame(errors.plot.RMSE)
errors.plot.RMSE$mod.size<-rep(row.names(errors),2)
errors.plot.RMSE$type<-c(rep("Cross Validation",28),rep("In Sample",28))
colnames(errors.plot.RMSE)[1]<-"RMSE"

# plots with RMSE for both insample and CV

plot.RMSE<-ggplot(errors.plot.RMSE,aes(x=as.numeric(mod.size),y=RMSE,color=type)) + geom_point() + geom_smooth(se=F) +
 xlab("Model Complexity (# of terms)") + ylab("RMSE")+
scale_x_continuous(breaks=c(seq(0,30,5))) + theme_bw() + theme(axis.text=element_text(face="bold"),axis.title=element_text(face="bold")) + scale_color_discrete(name="Source")

print(plot.RMSE)


#plot of just CV based MSE based R2
bvt.MSER2<-ggplot(errors,aes(x=as.numeric(mod.size),y=MSER2.CV)) + geom_point() + geom_smooth(se=F) +
 xlab("Model Complexity (# of terms)") + ylab(bquote(bold(R^2)))+
scale_x_continuous(breaks=c(seq(0,30,5))) + theme_bw() + theme(axis.text=element_text(face="bold"),axis.title=element_text(face="bold")) 

#bvt.MSER2

#plot of just CV RMSE
bvt.RMSE<-ggplot(errors,aes(x=as.numeric(mod.size),y=RMSE.pred.CV)) + geom_point() +geom_smooth(se=F) +xlab("Model Complexity (# of terms)") + ylab("RMSE") +scale_x_continuous(breaks=c(seq(0,30,5))) +theme_bw() + theme(axis.text=element_text(face="bold"),axis.title=element_text(face="bold")) 

#bvt.RMSE




```

# Practice Session

TODO:  Any Stata left to remove??
TODO:  Address ADD below

This section covers basic practice to be completed during the lab.   We are going to use the snapshot data described in Mercer et al 2011, discussed in class, and used last week.  It can be found on the class website.  Note that in this lab we are treating the errors are independent and identically distributed even though this assumption is not correct in these data.  (This is the same assumption typically made in LUR modeling.)

Perform the following tasks:
1.	Determine the R project you will use.
2.	Explore the dataset a bit, focusing on one particular season.  Make sure you have some basic understanding of the outcome variable (ln_nox), the CV grouping variable (cluster), and the large number of covariates you can choose from.   In this lab you should restrict your analysis to one season (e.g. winter).  
3.	Fit the model for one of the seasons given in Table 4 of Mercer et al. Make note of these in-sample estimates of R2 and RMSE.
4.	Try to manually cross-validate this model using the code given above.  Compare the CV R2 and RMSE to your in-sample estimates.  (Note:  Use the cluster variable in the dataset to define your CV groups.)
5.	Use the cross-validation function and repeat your cross-validation analysis.  (If you use the same groups, you should get the same results as in the previous step.)
6.	Make a scatterplot comparing ln_nox (the observed dependent variable) on the x-axis with the cross-validated predictions on the y-axis.  Add the 1:1 line to your plot. (If you also want to show the best-fit line, you’ll need to put the predictions on the x-axis rather than the y-axis.)
7.	Create your own version of the bias-variance trade-off plot shown in class using the following steps:
a.	Do a forward stepwise regression of ln_nox on a set of plausible variables with a lax entry criterion (e.g. ADD).   (You may restrict your attention to the list in the forward selection model example given above.)  Keep track of the order the variables were added.  
b.	Use the order of entry into the stepwise to sequentially complete the following computations.  For models with one up to the maximum number of selected variables:
i.	Use the full dataset to obtain in-sample estimates of the RMSE and R2.
ii.	Estimate predicted values using cross-validation.
iii.	Compute out-of-sample RMSE and MSE-based R2 estimates.
c.	In a table or figure(s), summarize the number of variables in each model along with the R2 and/or RMSE estimates from CV and the training data.   If you choose to show your results in a plot, put the number of variables in the model on the x-axis vs. the R2 or RMSE estimates on the y-axis.  Distinguish the two kinds of estimates on your plot.  If you choose to show your results in a table, also include a column for the variable name of the variable added.



# Homework Exercises

1.	Write a brief summary of the purpose of the lab and your approach.  Then present your results:

    a.  Describe the results (with appropriate displays in table(s) and/or figures(s)), and
    
    b.  Discuss the insights you have obtained from your analyses, both of the training data alone and after cross-validation. In your discussion, comment on how your in-sample and cross-validated MSE-based R2 estimates compare.
    
2.	**Extra credit**. Present one or both of the following results in your write-up:

    a.  Repeat the exercise using randomly defined CV groups that ignore the gradient clusters.
    
    b.  Repeat the exercise using yet another different set of CV groups, either chosen randomly, or spatially, or based on some other criterion. 



# Other resources to explore

TODO:  Update and edit.  Load appropriate packages. Remove extraneous ideas.

## Cross Validation using the R packages (cvTools and/or caret)
NOTE:  This has not been tested yet.  We need to determine whether this package gives the same results as our manual procedures, or not.  Also, the worked example below is randomly choosing CV groups, which is a very different approach from using the cluster variable.

Of the three commonly used cross-validation methods (random subsampling, k-fold cross-validation and leave-one-out cross-validation), here we will focus on k-fold cross-validation. For more details on cross-validation, please refer to the **Lecture Notes** and <https://www.r-bloggers.com/cross-validation-for-predictive-analytics-using-r/>.

The following uses tools and examples from *cvTools* package to write a program to cross-validate results from our linear model.

Set seed for reproducibility
```{r echo=TRUE}
set.seed(1234)
```

Set-up folds for cross-validation, using random allocation into groups.  (Recall the snapshot data has clusters already assigned that keep road gradient sites together.)
```{r set up CV folds, echo=TRUE,eval=FALSE}
folds <- cvFolds(nrow(fall), K = 10, R = 10)
```

Perform cross-validation for a least square regression model 
```{r perform CV, echo=TRUE,eval=FALSE}
fit_LM <- lm(ln_nox ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm, data = fall)
cv_results <- repCV(fit_LM, cost = rtmspe, folds = folds, trim = 0.1)
```

Summarize and plot results of cross-validation
```{r summarize CV, echo=TRUE, eval=FALSE}
summary(cv_results)
bwplot(cv_results)

plot(cv_results, method = "density")
```




### Cross-validation code from other packages

TODO: decide if we should drop this 

The `caret` package has code for k-fold cross-validation, as does the `boot` package.  and the `cvTools` package.  Not sure which to prioritize.

#### CV code from stackoverflow; uses a nnet

TODO:  probably drop

```{r}
set.seed(450)
cv.error <- NULL
k <- 10

library(plyr) 
pbar <- create_progress_bar('text')
pbar$init(k)

## Assign samples to K folds initially
index <- sample(letters[seq_len(k)], nrow(data), replace=TRUE)
for(i in seq_len(k)) {
    ## Make all samples assigned current letter the test set
    test_ind <- index == letters[[k]]
    test.cv <- scaled[test_ind, ]
    ## All other samples are assigned to the training set
    train.cv <- scaled[!test_ind, ]

    ## It is bad practice to use T instead of TRUE, 
    ## since T is not a reserved variable, and can be overwritten
    nn <- neuralnet(f,data=train.cv,hidden=c(5,2),linear.output=TRUE)

    pr.nn <- compute(nn,test.cv[,1:13])
    pr.nn <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)

    test.cv.r <- (test.cv$medv) * (max(data$medv) - min(data$medv)) + min(data$medv)

    cv.error[i] <- sum((test.cv.r - pr.nn) ^ 2) / nrow(test.cv)
       pbar$step()
}
    
```


# Appendix

```{r session.info}
#-----------------session.info: beginning of Appendix -----------------
#This allows reproducibility by documenting the version of R and every package you used.
sessionInfo()
```

```{r appendix.code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=T}

```


