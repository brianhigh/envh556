---
title: 'Week 2 Lab:  Testing questions'
author: "Lianne Sheppard for ENVH 556"
date: "Created Winter 2019; Updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        toc: true
        toc_depth: 3
        number_sections: true
---


```{r knitr.setup, include=FALSE, echo=FALSE}
#----knitr.setup-------------

knitr::opts_chunk$set(
    echo = TRUE,
    cache = TRUE,
    cache.comments = FALSE,
    message = FALSE,
    warning = FALSE
)

```

```{r clear.workspace, eval=FALSE, echo=FALSE}
#---------clear.workspace------------
# code to clear the environment without clearing knitr. Useful for code
# development because it simulates the knitr environment Run as a code chunk
# when testing.  When knitr is run, this is effectively run in the knitr
# environment

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
res <- suppressWarnings(lapply(
paste('package:', names(sessionInfo()$otherPkgs), sep = ""),
detach,
character.only = TRUE,
unload = TRUE,
force = TRUE
))

}

```

```{r load.libraries.with.pacman, include=FALSE}
#-----load.libraries.with.pacman--------------
# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  Some reasons for packages:
# knitr:  kable()
# tidyverse: multiple data science packages including ggplot, dplyr, readr
# Hmisc:  describe
# EnvStats: geoMean, geoSD, probability plotting functions
# MKmisc:  quantileCI
# codetools:  code analysis tools used in SessionInfo
pacman::p_load(tidyverse, knitr, Hmisc, EnvStats, MKmisc, codetools)

```

```{r read.data, echo=FALSE}
#-----------read.data-----------------
# getwd
ProjectPath <- getwd()

# dir.create    
dir.create(file.path(ProjectPath, "Datasets"),
           showWarnings = FALSE,
           recursive = TRUE)
           datapath <- file.path(ProjectPath, "Datasets")
           
# read.dat
# This assumes you already have the data copied into your data directory
DEMS <- readRDS(file.path(datapath, "DEMSCombinedPersonal.rds"))
           
```


# Set-up

```{r create.tibble}
#-----create.tibble----
DEMSt <- as_tibble(DEMS)

```

(@) **Keep a subset of observations:**  Keep only a subset of the data based on
selected jobs (240,110,410,600), underground only ("u"), and ecdata being
nonmissing:

```{r filter.data}
#-------filter.data----
# filter dataset so we only keep 4 jobs with codes 240,110,410,600; underground
# only ("u"); and ecdata being nonmissing
DEMSu <-
    filter(DEMSt, u_s == "u", ecdata > 0, job %in% c(240, 110, 410, 600))
    
```

(@) **Create new variables** using the log transformation.  Typically exposure
data appear to be log-normally distributed.

```{r transform.vars}
#-------transform.vars------
# The following two variables will be added to the DEMS dataframe at the end of
# the tibble:
DEMSu <- mutate(DEMSu,
                lnrec = log(ecdata))

# log base 10 transform
#DEMSu <- mutate(DEMSu,
#                log10rec = log10(ecdata))

```

(@) **Summarize variables and display key quantities:** This code uses `dplyr`,
part of `tidyverse`.  We use `group_by()` to determine the subgroups we are going to summarize over, `summarize()` to create new summaries we want to report, and `arrange()` to decide the final ordering in the table.  Each of these is connected though the pipe (`%>%`) which can be read as "then".  It facilitates the process of connecting multiple steps without creating intermediate datasets.

```{r table.with.dplyr}
#----table.with.dplyr------
DEMSsummary <- DEMSu %>%     group_by(job,facilityid) %>%
    dplyr::summarize(
    N = n(),
    AM = round(mean(ecdata), 2),
    AM_mle = round((exp(mean(lnrec)+((N-1)/N)*(sd(lnrec)^2)/2)), 2),
    ASD = round(sd(ecdata), 2),
    GM = round(geoMean(ecdata), 2),
    GSD = round(geoSD(ecdata), 2)
    ) %>%
arrange(job,facilityid,desc(AM))
    DEMSsummary

# And here is the same result printed using kable
kable(DEMSsummary)

```



## Take a random sample of data in memory 

* `small<-sample(DEMSu$lnrec,10)` gives a sample of size 10 of the data
* `half<- sample(DEMSu$lnrec,size=length(DEMSu$lnrec)/2,replace=TRUE)` gives a
50% sample of the data with replacement

```{r samples}
#---samples-------
# recall you want to set the seed to make your work reproducible
set.seed(502)

# the sample command takes from the vector of data (the first argument), a
# sample of size given by the argument.  The default is to sample without
# replacement, so you need to set replace=TRUE if you want sampling with
# replacement.
small <- sample(DEMSu$lnrec, size = 10)
half <- sample(DEMSu$lnrec,
               size = length(DEMSu$lnrec) / 2,
               replace = TRUE)

```


## Calculate the exceedence fraction and related statistics

Here we get the empiric eceedence fraction, the parametric exceedence
probability and show the tools for estimating various confidence limits for
percentiles

* **Empiric exceedence fraction**: 
```{r emp.exc.frac}
#-----emp.exc.frac----
sum(DEMSu$ecdata > 200) / length(DEMSu$ecdata)

```

* **Parametric eceedence fraction**: 

```{r param.exc.frac}
#--------param.exc.frac------
1 - pnorm((log(200) - mean(DEMSu$lnrec)) / sd(DEMSu$lnrec))

```

* **Upper 5th percentile of the distribution and its 70% CI**.  The following
describes an approach estimated directly on the log-transformed data and
exponentiates.  You can use the `quantileCI` command in the `MKmisc` package. (See
https://rdrr.io.)  The `quantile` command in base R can be used to get the upper
5th percentile (or 95th percentile), but there is no clear way to get its CI from
the `quantile` command.

```{r quantile95+70thCI}
#---------quantile95+70thCI-----
# Using the quantile command:
# 95th percentile quantile on log scale
quantile(DEMSu$lnrec, .95)

# 95th percentile quantile on native scale
exp(quantile(DEMSu$lnrec, .95))

# now using quantileCI:
quantileCI(DEMSu$lnrec, prob = 0.95, conf.level = 0.7)
quantileCI(DEMSu$ecdata, prob = 0.95, conf.level = 0.7)

```

## Log probability plots

These plots have the value of the untransformed exposure variable (e.g. concentration) on the x axis displayed on the log base 10 scale, and the corresponding normal probability for the cumulative distribution on the y axis.  See slide 18 in the lecture notes.  The following code generates some lognormal data and then plots them using this framework.  To implement this with the DEMS data, you will need to address specifics in the example, such as locations of the tick lines and range of the data.

Notes on how to create this: 

1. Focus on x, our exposure variable of interest, which is typically assumed to be lognormally distributed  

2. Transform data to the log scale (y=ln(x)) 

3. Generate order statistics (p_i=order/N+1)

4. Convert these order statistics to standard normal quantiles with the same mean and SD as y.

5. Exponentiate the normal quantile variable so it is comparable with x.

6. Plot the theoretical quantiles (exponentiated quantiles) vs the input data x (data on the x axis; theoretical quantiles on the y axis).

7. Plot labels on y axis shows the corresponding normal probabilities rather than the theoretical quantiles.  Both axes scale to the log base 10 scale. 

```{r generate data for log probability plot}
#----- generate data for log probability plot example ----
set.seed(2001)

# y is the lognormal; x is exp(y)~LN(mu_y,sd_y)
# generate a random normal, with specified mean and sd as read off slide 18
# eventually will use data not generated
# variable y is the normally distributed data
# for exposure data this is ordinarily the log-transformed measurement
y<-rnorm(1000,mean=1,sd=.92)

# x is the "measured" exposure data ~LN
x<-exp(y)

# Use rank to get the order statistics 
rx<-rank(x)

# order statistics re-expressed as proportions
p_i<-rx/(length(x)+1)

# corresponding normal quantiles for a distribution with realized mean and
# variance of y.  Need to use these later for the y axis probabilities.
# (Surrounding parentheses print these in the output if echo=TRUE)
(ybar <- mean(y))
(sd_y <- sd(y))

# theoretical quantiles of the normal distribution that corresponds to the data
# on the log scale
qy <-qnorm(p_i)*sd_y+ybar

#quantiles of the corresponding LN distribution
qx <- exp(qy)

# create a data frame for plotting
pplot.dat <- as.data.frame(cbind(y,x,rx,p_i,qy,qx))

# some summary statistics to check while developing this (commented out)
#summary(y)
paste("GM:  ", exp(ybar))
paste("GSD:  ",exp(sd(y)))
paste("AM:  ", exp(ybar+sd_y^2/2))
#summary(x)
#sd(x)

# now generate the data for the y axis -- need to create a vector of probabilities
# that we wish to display:
probs <- c(.01, .02, .05, .1, .16, .25, .5, .75, .84, .9, .95, .98, .99)

# get the corresponding quantiles for the normal distribution with the same mean and variance
quants <- qnorm(probs,mean=ybar,sd=sd_y)

# exponentiate these for plotting
exp_quants <- exp(quants)

# in the plots we will draw horizontal lines at exp_quants and label these lines with the probs

```

```{r log probabililty plot}
#---- log probability plot ----
# Shows percentiles of the cumulative normal on the y axis; 
# axes on the log base 10 scale;
# coord_fixed assumes both axes are the same scaling (i.e. the aspect ratio for
# x,qx is the same);
# annotation_logticks puts in minor ticks in right scaling
# minor_breaks addresses the unlabeled grid lines
p <- ggplot(data=pplot.dat) +
    geom_point(aes(x,qx) ) +
    geom_line(aes(qx,qx)) +
    annotation_logticks(sides="b") +
    scale_x_log10(breaks=c(0.1,0.5,1,5,10,50),
                  limits=c(0.1,50),
                  minor_breaks=c(0.2,1,2,20) ) +
    scale_y_log10(breaks=exp_quants,
                  labels=probs,
                  limits=c(0.1,50),
                  minor_breaks=NULL) +
    coord_fixed() +
    labs(title="Sample log probability plot\nUsing simulated data",
         x = "Concentration (native scale units)",
         y= "% of data less than") +
    theme_bw()
p

```

# Student questions  


## Table with exceedence fractions  


I am trying to create a table with the exceedence fractions by job. However, the coding below doesn't seem to create this by job; it still just gives me the total (overall) exceedence fraction: 

```
DEMSu %>%
group_by(job) %>%
summarise(empiric.exceedence = (sum(DEMSu$ecdata > 200) / length(DEMSu$ecdata)),
"95 perc" = exp(quantile(DEMSu$lnrec, .95)))
```
 
Also, I am unable to include the CI because it includes multiple values (a lower and upper bound). How can I manipulate this to include it in the table? 

 
I could also just calculate this manually, with the coding below, but then I'm not sure how to put that in a table with the 95% CI, etc:

 
```
sum(DEMSu$ecdata > 200 & DEMSu$job == 110) / length(DEMSu$ecdata & DEMSu$job == 110)
sum(DEMSu$ecdata > 200 & DEMSu$job == 240) / length(DEMSu$ecdata & DEMSu$job == 240)
sum(DEMSu$ecdata > 200 & DEMSu$job == 410) / length(DEMSu$ecdata & DEMSu$job == 410)
sum(DEMSu$ecdata > 200 & DEMSu$job == 600) / length(DEMSu$ecdata & DEMSu$job == 600) 
```

```{r test quantileCI}
# test quantileCI ----------

# now using quantileCI:
#quantileCI(DEMSu$lnrec, prob = 0.95, conf.level = 0.7)
lwr <- quantileCI(DEMSu$ecdata, prob = 0.95, conf.level = 0.7)$conf.int[1]
upr <- quantileCI(DEMSu$ecdata, prob = 0.95, conf.level = 0.7)$conf.int[2]
lwr
upr
```


```{r test the table with CI}
# test the table with CI -----

DEMSu %>%
    group_by(job) %>%
    dplyr::summarise(
        n = n(),
        empiric.exceedence = (sum(ecdata > 200) / length(ecdata)),
        "95 perc" = exp(quantile(lnrec, .95)),
        "lower 70% CI" = quantileCI(ecdata, prob = 0.95, conf.level = 0.7)$conf.int[1],
        "upper 70% CI" = quantileCI(ecdata, prob = 0.95, conf.level = 0.7)$conf.int[2])
            
```

## Filtering by number of observations

Re: dropping rows with less than a certain number of observations. 

It looks like you can create filters fairly easily in dplyr, see the explanation here: (not secure; Firefox encouraged not connecting to the following website)  https://blog.exploratory.io/filter-data-with-dplyr-76cf5f1a258e

However, this doesn't seem to be working for me when I specify the condition as N>5, for example. (filter(N >= 5)). If anyone gets this working, let me know! 

Suggestion:  You may be able to use filter(nrow(MyDataSetWithoutNAs) >=5) ?

```{r table.with.Nfiltering}
#----table.with.Nfiltering------
DEMSsummary <- DEMSu %>%     group_by(job,facilityid) %>%
    filter(n() >= 5) %>%
    dplyr::summarize(
        N = n(),
        AM = mean(ecdata),
        AM_mle = (exp(mean(lnrec)+((N-1)/N)*(sd(lnrec)^2)/2)),
        ASD = sd(ecdata),
        GM = geoMean(ecdata),
        GSD = geoSD(ecdata)
        ) %>%
    arrange(facilityid,desc(AM))
    
kable(DEMSsummary, digits = 2)

```

## Log probabiity plot for a subset of the DEMS data 

What needs to be modified to get this to work?  See below.

```{r log probabililty plot using DEMS}
#---- log probability plot using DEMS----
# Shows percentiles of the cumulative normal on the y axis; 
# axes on the log base 10 scale;
# coord_fixed assumes both axes are the same scaling (i.e. the aspect ratio for
# x,qx is the same);
# annotation_logticks puts in minor ticks in right scaling
# minor_breaks addresses the unlabeled grid lines

# first set up the variables we need to use in the plot
# use the DEMS data:
# x is the "measured" exposure data ~LN
# choose job 400 from facility G here (because a large N)
newdat <- DEMSu %>%
    filter(job == 410, facilityid == "G", u_s == "u", ecdata > 0) %>%
    dplyr::select(ecdata)
# save in x as a vector to simplify and make the plot easier to manage
x <- as.vector(newdat$ecdata)
y <- log(x)

# Use rank to get the order statistics 
rx<-rank(x)

# order statistics re-expressed as proportions
p_i<-rx/(length(x)+1)

# corresponding normal quantiles for a distribution with realized mean and
# variance of y.  Need to use these later for the y axis probabilities.
# (Surrounding parentheses print these in the output if echo=TRUE)
(ybar <- mean(y))
(sd_y <- sd(y))

# theoretical quantiles of the normal distribution that corresponds to the data
# on the log scale
qy <-qnorm(p_i)*sd_y+ybar

#quantiles of the corresponding LN distribution
qx <- exp(qy)

# create a data frame for plotting
pplot.dat <- as.data.frame(cbind(y,x,rx,p_i,qy,qx))

# some summary statistics to check while developing this (commented out)
#summary(y)
paste("GM:  ", exp(ybar))
paste("GSD:  ",exp(sd(y)))
paste("AM:  ", exp(ybar+sd_y^2/2))
paste("max:  ", max(x))
paste("min:  ", min(x))
#summary(x)
#sd(x)

# now generate the data for the y axis -- need to create a vector of probabilities
# that we wish to display:
probs <- c(.01, .02, .05, .1, .16, .25, .5, .75, .84, .9, .95, .98, .99)

# get the corresponding quantiles for the normal distribution with the same mean and variance
quants <- qnorm(probs,mean=ybar,sd=sd_y)

# exponentiate these for plotting
exp_quants <- exp(quants)

# in the plots we will draw horizontal lines at exp_quants and label these lines with the probs

# now produce the plot
# from the class example, I modified:
#   1. the breaks in x
#   2. the minor breaks in x
#   3. the limits in both
p <- ggplot(data=pplot.dat) +
    geom_point(aes(x,qx) ) +
    geom_line(aes(qx,qx)) +
    annotation_logticks(sides="b") +
    scale_x_log10(breaks=c(0.1,0.5,1,5,10,50,100),
                  limits=c(1,250),
                  minor_breaks=c(0.2,1,2,20,200) ) +
    scale_y_log10(breaks=exp_quants,
                  labels=probs,
                  limits=c(1,250),
                  minor_breaks=NULL) +
    coord_fixed() +
    labs(title="Sample log probability plot\nUsing DEMS REC from facility G, job 410",
         x = "Concentration (ug/m3)",
         y= "% of data less than") +
    theme_bw()
p

```

# Appendix:  Code and session information {-}

```{r session.info}
#------------session.info: beginning of Appendix ------
# This allows reproducibility by documenting the version of R and every package
# you used.
sessionInfo()

```

```{r appendix.code, ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE, , include=TRUE}
# ---------appendix------------
    
```

