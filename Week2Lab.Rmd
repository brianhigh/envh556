---
title: 'Week 2 Lab:  Compliance -- Descriptive Statistics and Exceedence Probabilities'
author: "Lianne Sheppard for ENVH 556"
date: "Created Winter 2019; Updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        toc: true
        toc_depth: 3
        number_sections: true
---

<!--
TODO:  Verify OEL=occupational exposure limit
TODO: Verify exceedences are correct.
TODO:  Work on a probability plot


-->


```{r knitr.setup, include=FALSE, echo=FALSE}
#----knitr.setup-------------

knitr::opts_chunk$set(
    echo = TRUE,
    cache = TRUE,
    cache.comments = FALSE,
    message = FALSE,
    warning = FALSE
)

```

```{r clear.workspace, eval=FALSE, echo=TRUE}
#---------clear.workspace------------
# code to clear the environment without clearing knitr
# useful for code development because it simulates the knitr environment Run as
# a code chunk when testing.  When knitr is run, this is effectively run in the
# knitr environment

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
res <- suppressWarnings(lapply(
paste('package:', names(sessionInfo()$otherPkgs), sep = ""),
detach,
character.only = TRUE,
unload = TRUE,
force = TRUE
))

}

```

```{r load.libraries.with.pacman, include=FALSE}
#-----load.libraries.with.pacman--------------
# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  Some reasons for packages:
# knitr:  kable()
# tidyverse: multiple data science packages including ggplot, dplyr, readr
# Hmisc:  describe
# EnvStats: geoMean, geoSD, probability plotting functions
# MKmisc:  quantileCI
# codetools:  code analysis tools used in SessionInfo
pacman::p_load(tidyverse, knitr, Hmisc, EnvStats, MKmisc, codetools)

```

```{r read.data, echo=FALSE}
#-----------read.data-----------------
# getwd
ProjectPath <- getwd()

# dir.create    
dir.create(file.path(ProjectPath, "Datasets"),
           showWarnings = FALSE,
           recursive = TRUE)
           datapath <- file.path(ProjectPath, "Datasets")
           
# read.dat
DEMS <- readRDS(file.path(datapath, "DEMSCombinedPersonal.rds"))
           
```

# Purpose  

The purpose of this lab is to work with descriptive statistics, and compliance
tests while also getting further practice with R, RStudio, and R Markdown.  We
will use the DEMS REC data, describe the distributions, exceedence
probabilities, confidence intervals and test compliance.

# Getting Started  
This section gives formulas and some basic R commands for descriptive statistics
and exceedance probabilities.  It also provides reminders of formulas from
lecture you will use in lab.

## Definitions

* AM=arithmetic mean  
* GM=geometric mean  
* SD=standard deviation (on the native scale)
* GSD=geometric standard deviation
* MOM=method of moments
* MLE=maximum likelihood estimate
* OEL=occupational exposure limit
* CI=confidence interval

## Formulas for method of moments vs MLE estimates for the AM  

* AM method of moments (MOM) estimate for ecdata (called x in equation): 
$$x_{MOM} = \frac{1}{n} \sum_{i=1}^{n} x_i $$

* AM maximum likelihood estimate (MLE) for `ecdata`=$x$ using
`lnrec`=`log(ecdata)`=$y$:
$$ AM_{MLE}=\exp\big(\mu_y+\frac{\sigma_y^2}{2}\big)$$
$$ \bar{x}_{MLE}=\exp{\big(\bar{y}+\frac{\frac{N-1}{N}s_y^2)}{2}\big)}$$

## Formulas for the exceedence fraction  
* Empiric exceedence fraction:  For a sample of size *N* where *n* is the number
exceeding the OEL, calculate
$$f_{OEL}=\frac{n>\mbox{OEL}}{N}$$
* Parametric exceedence fraction:  For a sample, log-transform the data and OEL
and use the normal distribution to estimate the probability  of exceeding
ln(OEL):
$$P\big(y>\ln(OEL)\big) =
\big(1-\Phi(z=\frac{y-\bar{y}}{s_y}>\frac{\ln(OEL)-\bar{y}}{s_y})\big)$$

## Basic data manipulation and summarization commands   
(See also Week 1 lab)

@ First let's turn our DEMS dataset into a tibble.
```{r create.tibble}
#-----create.tibble----
DEMSt <- as_tibble(DEMS)

```

@ **Keep a subset of observations:**  Keep only a subset of the data based on
selected jobs (240,110,410,600), underground only("u"), and ecdata being
nonmissing:

```{r filter.data}
#-------filter.data----
# filter dataset so we only keep 4 jobs ( 240,110,410,600); underground
# only("u"); and ecdata being nonmissing
DEMSu <-
    filter(DEMSt, u_s == "u", ecdata > 0, job %in% c(240, 110, 410, 600))
    
```

@ **Create new variables** using the log transformation.  Typically exposure
data appear to be log-normally distributed.

```{r transform.vars}
#-------transform.vars------
# The following two variables will be added to the DEMS dataframe at the end of
# the tibble:
DEMSu <- mutate(DEMSu,
                lnrec = log(ecdata))

# log base 10 transform
DEMSu <- mutate(DEMSu,
                log10rec = log10(ecdata))

```

@ **Summarize variables and display key quantities:** This code uses `dplyr`,
part of `tidyverse`.

```{r table.with.dplyr}
#----table.with.dplyr------
DEMSsummary <- DEMSu %>% group_by(facilityno, job) %>%
    dplyr::summarize(
    N = n(),
    AM = round(mean(ecdata), 2),
    ASD = round(sd(ecdata), 2),
    GM = round(geoMean(ecdata), 2),
    GSD = round(geoSD(ecdata), 2)
    )
    DEMSsummary

# And here is the same result printed using kable
kable(DEMSsummary)

```

## Visualize the data and understand its distribution graphically

Here are some sampe graphical commands (not run and histograms repeated from
Week 1 lab):

### Histograms

```{r hist.in.tidyverse, echo=TRUE,eval=FALSE}
#------------hist.in.tidyverse-----------
# first define the basic plot using the density scale
p <- ggplot(data = DEMSu, aes(lnrec)) +
    geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = 0.5
    )

# create variables to overlay a normal density curve, next 3 commands:
N <- n(DEMSu$lnrec)

# divides the range 0-1000 into N equal increments
x <- seq(min(DEMSu$lnrec), max(DEMSu$lnrec), length.out = N)
df <-
    with(DEMS, data.frame(x, y = dnorm(x, mean(lnrec), sd(lnrec))))

# histogram + overlaid normal + density w/ transparency
p +
    geom_line(data = df, aes(x = x, y = y), color = "red")  +
    geom_density(alpha = .2, fill = "red")
    
```

### Q-Q plots (Normal probability plots)

```{r qqplot.in.tidyverse, echo=TRUE,eval=FALSE}
#--------------qqplot.in.tidyverse-------
# create the base plot
p <- ggplot(DEMSu, aes(sample = lnrec))

# now overlay the ppoints and line
# and add a title
p + stat_qq() + stat_qq_line() +
    labs(title = "Normal Q-Q Plot of ln(REC)\nIn 4 Underground Jobs")
```

##  Test the normality of a variable 

```{r Shapiro-Wilk test}
#------------Shapiro-Wilk test-----------
#for normality of a variable
shapiro.test(DEMSu$lnrec)

```

## Take a random sample of data in memory 

* `small<-sample(DEMSu$lnrec,10)` gives a sample of size 10 of the data
* `half<- sample(DEMSu$lnrec,size=length(DEMSu$lnrec)/2,replace=TRUE)` gives a
50% sample of the data with replacement

```{r samples}
#---samples-------
small <- sample(DEMSu$lnrec, size = 10)
half <- sample(DEMSu$lnrec,
               size = length(DEMSu$lnrec) / 2,
               replace = TRUE)

```


## Calculate the exceedence fraction and related statistics

Here we get the empiric eceedence fraction, the parametric exceedence
probability and show the tools for estimating various confidence limits for
percentiles

TODO: Verify these are correct.

* **Empiric exceedence fraction**: 
```{r emp.exc.frac}
#-----emp.exc.frac----
sum(DEMSu$ecdata > 200) / length(DEMSu$ecdata)

```

* **Parametric eceedence fraction**: 

```{r param.exc.frac}
#--------param.exc.frac------
1 - pnorm((log(200) - mean(DEMSu$lnrec)) / sd(DEMSu$lnrec))

```

* **Upper 5th percentile of the distribution and its 70% CI**.  The following
describes an approach estimated directly on the log-transformed data and
exponentiates.  You can use the `quantileCI` command in the `MKmisc` package see
https://rdrr.io.  The `quantile` command in base R can be used to get the upper
5th percentile (or 95th percentile), but there is clear way to get its CI from
the `quantile` command.

```{r quantile95+70thCI}
#---------quantile95+70thCI-----
# Using the quantile command:
# 95th percentile quantile on log scale
quantile(DEMSu$lnrec, .95)

# 95th percentile quantile on native scale
exp(quantile(DEMSu$lnrec, .95))

# now using quantileCI:
quantileCI(DEMSu$lnrec, prob = 0.95, conf.level = 0.7)
quantileCI(DEMSu$ecdata, prob = 0.95, conf.level = 0.7)

```


# Practice Session  

1.	Make sure you have started a new project for this lab.
2.	Read in the ‘DEMSCombinedPersonal’ R dataset and keep only measurements from
jobs 110, 240, 410 and 600 among underground workers.  We have selected these
particular jobs for simplicity, but feel free to explore additional jobs or
other categories of the data if you want to.  In order to avoid potential
confusion later, also drop any observations that are missing ecdata.
3.	Describe REC (varname: ecdata)
4.	Determine whether REC in this subset is lognormally distributed.  Explore
using histograms, qqplots,  and statistical tests.
5.	Calculate the GM, GSD, and AM (using both method of moments and maximum
likelihood estimates for the AM) for each group.
6.	 For a selected group (i.e.,a single job, or for all four if you want):
Assume data are lognormal (LN) and an OEL of 200 $\mu$g/m^3^ has been determined
for REC.

    a)	Calculate the empiric and parametric exceedence probabilities along with
    the 95% percentile +70th % CI.
    
    b)	Take a random sample of 50%, 25%, n=9 and n=5 samples and recalculate the
    GM, GSD, 95% percentile $\pm$ 70 percent confidence limit CI.

# Homework Exercises

1) Consider the primary exposure measures of interest to the study, including
REC, NO_2_ and Organic Carbon.
    a) Describe the distribution, out of range and/or missing values.
    b) Determine the adequacy of the LN distribution for representing these using
distribution plots and/or statistical tests.
2) Explore potential stratification variables (determinants).
    a) Do the stratified data improve the distributional characteristics?
3) Calculate the GM, Median, AM (method of moments) and AM (maximum likelihood)
for REC.
    a) How do these parameters compare to each other?
    b) Can you determine any characteristics of the data which help explain the
differences between these alternative measures of central tendency?
4) Assuming an OEL for REC of 200 $\mu$g/m^3^, calculate the exceedence
probability for each mine (and/or job) using both empiric and parametric
approaches.   In addition, calculate the 95 percentile of the distribution, and
provide 70% confidence limits on these percentiles.
    a) What are the differences between the methods of calculation?
5) Take random samples of the data (e.g., 50%, 10%, n=9, n=5) and recalculate
the distributional parameters.
    a) How does the reduced sample size affect the parameters?  Explain.



# Appendix 1:  Older Base R versions for reference {-}

## A1.1 Q-Q plot {-}

For a q-q plot to determine whether the data are normally distributed, use `qqnorm()` and you can overlay `qqline()`.
```{r qqplot.in.baseR, echo=T}
#---------qqplot.in.baseR-----------
# apparently qqnorm works fine in the presence of missing data:
qqnorm(DEMS$ecdata)
qqline(DEMS$ecdata, col = "red", lwd = 2)

# now repeat using the DEMSu subset of data since they should be closer to
# normal
qqnorm(DEMSu$ecdata)
qqline(DEMSu$ecdata, col = "red", lwd = 2)

```


# Appendix 2:  Code and session information {-}

```{r session.info}
#------------session.info: beginning of Appendix ------
# This allows reproducibility by documenting the version of R and every package
# you used.
sessionInfo()

```

```{r appendix.code, ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE, , include=TRUE}
# ---------appendix------------
    
```

