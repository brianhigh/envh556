---
title: "Week 9 Lab:  Geostatistics"
author: "Lianne Sheppard for ENVH 556"
date: "Winter 2019; Updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        fig_caption: yes
        toc: true
        toc_depth: 3
        number_sections: true
editor_options: 
  chunk_output_type: console
---

<!--Basic document set-up goes here  -->

```{r setup, include=FALSE}
#-------------r.setup-------------
knitr::opts_chunk$set(echo = TRUE)

par.orig <- par()
```

```{r clear.workspace, eval=FALSE, echo=FALSE}
#---------clear.workspace------------
# Clear the environment without clearing knitr
#
# This chunk is useful for code development because it simulates the knitr
# environment. Run it as a code chunk when testing. When knitr is run, it uses a
# fresh, clean environment, so we set eval=FALSE to disable this chunk when
# rendering.

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```


```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}
#----------------load.libraries.pacman----
# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  Some reasons for packages:
# knitr:  kable()
# ggplot2: part of tidyverse
# readr: part of tidyverse
# dplyr: part of tidyverse
# multcomp:  glht
# modelr:  part of tidyverse and need for add_predictions and add_residuals
# boot:  cv tools are available
# Hmisc:  describe
# lme4:  for lmer, mixed models and random effects models
# parallel:  for parallel processing
# geoR:  for kriging
# maps: for maps
# sf:  ??
# maptools: ??
# scatterplot3d for the scatter3d option in geoR plotting
# funModeling:  for EDA
# scales: muted and other color scale functions
pacman::p_load(tidyverse, knitr, dplyr, geoR, maps, scatterplot3d,ggmap, funModeling, Hmisc, scales)

```

```{r read.data.from.a.web.site, eval=TRUE, echo=FALSE}
#-----read.data.from.a.web.site--------
# Download the data file from a web site if it is not already downloaded, then
# read in the file

datapath <- "Datasets"
dir.create(datapath, showWarnings=FALSE, recursive = TRUE)

snapshot.file <- "allseasonsR.rds"
grid.file <- "la_grid.csv"
snapshot.path <- file.path(datapath, snapshot.file)
grid.path <- file.path(datapath, grid.file)

# Only download the file if it's not already present
if (!file.exists(snapshot.path)) {
    url <- paste("https://staff.washington.edu/high/envh556/Datasets", 
                 snapshot.file, sep = '/')
    download.file(url = url, destfile = snapshot.path)
}

# Output a warning message if the file cannot be found
if (file.exists(snapshot.path)) {
    snapshot <- readRDS(file = snapshot.path)
} else warning(paste("Can't find", snapshot.file, "!"))

# note:  la_grid not on website yet
if (file.exists(grid.path)) {
    la_grid <- read_csv(file = grid.path)
} else warning(paste("Can't find", grid.file, "!"))

```

TODO:  search for TODOs and ?? and fix
TODO:  Address the practice session, homework exercises, etc.
TODO:  figure out maps

TODO:  where is the grayscale sequence definition from
    

# Introduction and Purpose

The purpose of this lab is to learn about geostatistical models and further solidify your understanding of regression for prediction.  We will use the same MESA Air snapshot data described in Mercer et al 2011 that we used earlier in the quarter.

**Important Winter 2019 note**:  This lab has not been fully debugged.  There are a number of ideas that need further investigation.  Most likely some additional newer packages should replace some of what is shown in this lab.  The intent is to make this as useful as possible while recognizing its limitations.

# Getting Started

## Spatial data in R:  Resources

TODO:  ADD

## R packages for geostatistics

The packages I am aware of include `geoR` and `gstat`.  `geoR` has been around for some time; `gstat` appears to be newer and has been updated more recently (2018 vs. 2016 for `geoR`).  The two appear to give identical results, at least for a simple dataset example.  To verify this, type `demo(gstat::comp_geoR)` in the console.

Because of my familiarity, we discuss `geoR` in this lab.

## Some comments on universal kriging and prediction:  

In kriging, you can't predict on the same locations that you used to estimate the parameters.  As discussed on [stackoverflow](https://stackoverflow.com/questions/45768516/why-does-the-kriging-give-the-same-values-as-the-observed), "this is a well-known property of kriging; it comes from the fact that the model underlying kriging assumes that a value is perfectly correlated with itself."  Thus predicting a know value always returns that value, with zero prediction error.  This is also the cause of errors with duplicate observations at the same location. Think of this property of kriging as an enforced need to do cross-validation to evaluate your predictions.  Apparently you can use the `gstat` package to do smoothing instead of kriging by specifying an Err variogram instead of a Nug nugget effect.

## Practice using `geoR`:

The purpose of this section is to show some `geoR` tools using the geo datasets included in the `geoR` package.  Both are stored in the geodata format.  First we go through the `s100` dataset, a simple dataset of simulated data.  This allows us to demonstrate the geodata object, the plot default, variograms, modeling using different approaches, kriging predictions, and plotting the results.  Then we explore `ca20` to understand inclusion of covariates into geostatistical analysis.

TODO:  Refine the practice using Sun's code.  Fix the legend problem.  Maybe figure out how to use ggmap with this??

### s100 dataset, a simulated dataset

#### Summary

```{r basics with s100 dataset - summary + EDA}
# basics with s100 dataset - summary -----------
# Summary uses the summary.geodata function
summary(s100)

# Plot uses the plot.geodata function in geoR, the default
#   the first plot is the data on an x-y "map" by quartiles of values
#   the next two plots show the data vs the x or y coordinate (to look for broad
#       trends by the coordinates)
#   the last plot shows a histogram of the data
plot(s100)
# need to install package scatterplot3d for the following to work
#   adds a 3d scatterplot as the 4th plot instead of the histogram
plot(s100, scatter3d = TRUE)

# points uses the points.geodata function in geoR
par(mfrow=c(2,2))
points(s100, xlab="X", ylab="Y", main="Map 1, default w/ sizes proportional to data")
points(s100, xlab="X", ylab="Y", pt.divide="rank.prop", main = "Map 2, point sizes proportional to data rank")
# TODO: Find out why the gray scale is set up as a sequence from 1 - .1 of length 100.  Presumably the length is because the data are 100 points.
points(s100, xlab="X", ylab="Y", cex.max=1.7, 
       col=gray(seq(1, 0.1, l=100)), pt.divide="equal", main = "Map 3, equal size points in gray scale")
points(s100, pt.divide = "quintile", xlab="X",ylab="Y", main = "Map 4, quintiles of the data")
```

#### Empirical variograms

The following code gives empirical variograms, computed using the default classical estimator, using two different options:  a *variogram cloud* with all squared distances (`option = "cloud"`), and the default *binned variogram* (`option = "bin"`).  With the bin option, we can specify the vector of breaks (`uvec`), or use the program defaults.  It is often good practice to also define the maximum distance (`max.dist`) to be about half the total distince since the stability of a variogram estimate breaks down as the distance gets close to the maximum.

```{r s100 empirical variogram}
# s100 empirical variogram ------------
cloud1 <- variog(s100, option="cloud", max.dist=1)
bin1 <- variog(s100, uvec=seq(0, 1, l=11))
par(mfrow=c(1,2))
plot(cloud1, main="Empirical variogram: cloud")
lines(smooth.spline(cloud1$u, cloud1$v, df = 6), col="red",lwd = 4)
plot(bin1, main="Empirical variogram: bin")

```

#### Modeled variagram

One can superimpose various modeled variograms onto empirical variograms.  This is important because we need to have some idea of appropriate variogram parameters because we need to supply the estimation fuctions (`variofit` and/or `likfit`) with initial values of these parameters, specifically the partial sill ($\sigma^2$) and range ($\phi$).  One can also allow for a non-zero nugget ($\tau^2$) parameter by using the `nugget = ` option with an estimated value after the "=".  The program defaults to estimating the nugget unless you use the `fit.nugget = TRUE` option.

```{r s100 modeled variogram}
# s100 modeled variogram -------------

# no nugget
plot(variog(s100, uvec=seq(0,1,l=11)),
     main = "Variogram fits, no nugget")
lines.variomodel(cov.model="exp", cov.pars=c(1, 0.3), nug=0, max.dist=1, lty = 1)
lines.variomodel(cov.model="exp", cov.pars=c(0.5, 0.3), nug=0, max.dist=1, lty = 2)
lines.variomodel(cov.model="exp", cov.pars=c(1, 0.6), nug=0, max.dist=1, lty = 3)
legend("bottomright", c("exp (1, 0.3)", "exp (0.5, 0.3)", "exp (1, 0.6)"), lty = 1:3, text.width = 0.2, cex = 0.6, seg.len = 1)


# with nugget
plot(variog(s100, uvec=seq(0,1,l=11)), 
     main = "Variogram fits, nugget = 0.2")
lines.variomodel(cov.model="exp", cov.pars=c(1, 0.3), nug=0.2, max.dist=1, lty = 1)
lines.variomodel(cov.model="exp", cov.pars=c(0.5, 0.3), nug=0.2, max.dist=1, lty = 2)
lines.variomodel(cov.model="exp", cov.pars=c(1, 0.6), nug=0.2, max.dist=1, lty = 3)
legend("bottomright", c("exponential (1, 0.3)", "exponential (0.5, 0.3)", "exponential (1, 0.6)"), lty = 1:3, text.width = 0.4, cex = 0.6, seg.len = 1)


```

##### How do we decide what is a reasonable variogram model?

This requires some investigation.  We can use a guess and check approach by fitting a set of different guesses as we show above.  Another way is to use the `eyefit` function.  This is an interactive function that will return estimated parameters.  HOWEVER, it does not seem to work in RStudio.  It does work in R if you want to open an R window.  Directly in R, try it by using the empirical variogram results and typing this command into the R gui:  `eyefit(bin1)`. 

Since we need to supply a likelihood fit with initial values of parameters, an alternative is to use the `variofit` function with no initial values and let `geoR` try to estimate these as is shown next.

```{r s100 estimate covariance parameters}
# s100 estimate covariance parameters ----------

# uses weighted least squares and an exponential variogram
wls_ests <- variofit(bin1, cov.model = "exp")

# override the default of 0 nugget to start (appears to give the same result)
wls_ests_nug <- variofit(bin1, cov.model = "exp", nugget = 0.5)

```

#### Parameter estimation

This example shows that we can get a likelihood fit using either by directly specifying the known (guessed) initial values for the partial sill and range (using the option `ini = c(partial_sill, range)`), or taken from the output of the `variog` function (using the option `ini = variog_object_name`.  Observe that we get the same result even though the initial parameter values are different.  Note that it is good practice to make sure the result of the optimization is not sensitive to the initial values.

```{r s100 parameter estimation using ML}
# s100 parameter estimation using ML ---------

# fit using maximum likelihood using initial parameter values
ml <- likfit(s100, ini = c(1, 0.5))
ml
summary(ml)

# repeat ML fit, now taking initial estimates from the wls variog fit
ml2 <- likfit(s100, ini = wls_ests)
ml2 
summary(ml2)

```

The following gives more examples of estimating parameters using various estimation methods.  The two basic options are `likfit` which uses maximum likelihood to do the estimation (with options for ML vs REML), and `variofit` which uses a parametric model fit using either ordinary or weighted least squares.  We also include plots to compare these results.  The plots show that the different approaches to estimation give different fitted variogram results.

```{r s100 parameter estimation examples}
# s100 parameter estimation examples ------------

# zero no nugget
ml <- likfit(s100, ini=c(1, 0.5), fix.nugget=T)
reml <- likfit(s100, ini=c(1, 0.5), fix.nugget=T, lik.method="REML")
ols <- variofit(bin1, ini=c(1, 0.5), fix.nugget=T, weights="equal")
wls <- variofit(bin1, ini=c(1, 0.5), fix.nugget=T)

# estimated nugget
ml.n <- likfit(s100, ini = c(1, 0.5), nugget = 0.5)
reml.n <- likfit(s100, ini = c(1, 0.5), nugget = 0.5, 
                 lik.method = "RML")
ols.n <- variofit(bin1, ini = c(1, 0.5), nugget = 0.5, 
                  weights = "equal")
wls.n <- variofit(bin1, ini = c(1, 0.5), nugget = 0.5)

par(mfrow=c(1, 2))
plot(bin1, main=expression(paste("fixed nugget:  ", tau^2==0)))
lines(ml, max.dist=1)
lines(reml, lwd=2, max.dist=1)
lines(ols, lty=2, max.dist=1)
lines(wls, lty=2, lwd=2, max.dist=1)
legend("bottomright", legend = c("ML","REML","OLS","WLS"), lty=c(1,1,2,2), lwd=c(1,2,1,2), cex=0.5, text.width = 0.1, seg.len = 2)

plot(bin1, main=expression(paste("estimated ", tau^2)))
lines(ml.n, max.dist=1)
lines(reml.n, lwd=2, max.dist=1)
lines(ols.n, lty=2, max.dist=1)
lines(wls.n, lty=2, lwd=2, max.dist=1)
legend("bottomright", legend = c("ML","REML","OLS","WLS"), lty=c(1,1,2,2), lwd=c(1,2,1,2), cex=0.5, text.width = 0.1, seg.len = 2)

```

#### Cross-validation -- leave one out (LOO)

`geoR` defaults to doing leave-one-out cross-validation (option `locations.xvalid = all`).  It also has an option for validating on an external dataset when the `data = ` option is set.  The alternative approach in the documentation, using a subset of the data selected with `locations.xvalid`, is not documented and thus it isn't clear how it works.  Thus in a later chunk we will enforce 10-fold cross-validation using a grouping variable and dataset splitting procedure.  (See *s100 kriging + 10-fold CV*.)  This strategy will be useful for the snapshot dataset when we have a pre-defined cluster variable. 

```{r s100 LOO cross-validation}
# ------------ s100 LOO cross-validation ----------------
# 
# Do cross-validation using two different models:  The likelihood fit model and the weighted least squares model.
xv.ml <- xvalid(s100, model=ml)
xv.wls <- xvalid(s100, model=wls)

# summarize the MSE and R2
mse.ml <- mean ((xv.ml$data - xv.ml$predicted)^2 )
mse.wls <- mean ((xv.wls$data - xv.wls$predicted)^2 )

# dropping the following because uses variance, not mean squared variations in
# the denominator.  Best to not correct for the d.f. as the variance does
#r2.ml <- 1 - mse.ml/var(xv.ml$data)
#r2.wls <- 1 - mse.wls/var(xv.wls$data)
r2.ml <- 1 - mse.ml/mean((xv.ml$data - mean(xv.ml$data))^2)
r2.wls <- 1 - mse.wls/mean((xv.wls$data - mean(xv.wls$data))^2)

# print the results
rbind(c("ML ", sqrt(mse.ml), r2.ml),
      c("WLS", sqrt(mse.wls), r2.wls))

# uses `plot.xvalid` to produce a set of plots showing the cross-validation results
# under the weighted least squares variogram model
par(mfcol=c(5,2), mar=c(3,3,1,0.5), mgp=c(1.5,0.7,0))
plot(xv.wls)

# under the maximum likelihood variogram model
par(mfcol=c(5,2), mar=c(3,3,1,0.5), mgp=c(1.5,0.7,0))
plot(xv.ml)

```


#### Kriging (ordinary)

We use kriging to get predictions at *new* locations (not used in the model fitting).  Use the function `krig.conv` to accomplish this.  Give it locations where it should predict in the `locations = ` option.  You also need to pass it the results of a fitted model in the `krige = ` option.

```{r s100 kriging}
# s100 kriging --------------
 
# Set up the plot area
par(mfrow = c(2,2))

# show the coordinates and the 4 prediction locations, 
#   one location is outside the area of the data
plot(s100$coords, xlim=c(0,1.2), ylim=c(0,1.2), xlab="X", ylab="Y",
     main = "Data locations + 4 new loci to predict at")
loci <- matrix(c(0.2,0.6,0.2,1.1,0.2,0.3,1,1.1), ncol=2)
text(loci, as.character(1:4), col="red")

# conventional Kriging
# Predict at the 4 loci defined above
#   Note:  one location is out of the data area
# Kriging default is ordinary kriging (OK; i.e. with constant mean)
kc4 <- krige.conv(s100, locations=loci, krige=krige.control(obj.m=wls))

# find out what is in the object produced
names(kc4)

# print the values of the predictions
kc4$predict

# print the variances of the predictions
kc4$krige.var

# rerun the kriging predictions, now based on the likelihood fit and to predict
# on a grid we create:
pred.grid <- expand.grid(seq(0,1,l=51), seq(0,1,l=51))
kc <- krige.conv(s100, loc=pred.grid, krige=krige.control(obj.m=ml))

# define colors for plotting
# TODO:  look into ColorBrewer
col.range <- c("darkblue","blue","green","yellow","orange","red")

# add prediction plots, 3 types:
# TODO:  As Sun about choices here
# TODO:  Set xlim, ylim so plots are square?  getting funny limits
# floor used to get quick and dirty quartiles based on cts data
plot(s100$coords[,1:2], col=col.range[c(2:4,6)][floor(s100$data+2)], xlab="X", ylab="Y", main = "Data in categories")

plot(pred.grid[,1:2], col=col.range[c(2:4,6)][floor(kc$predict+2)], xlab="X", ylab="Y", main = "Grid predictions in categories")

image(kc, loc=pred.grid, col=gray(seq(1,0.1,l = 30)), xlab="X", ylab="Y", main = "Grid predictions as gray-scale image")

```

#### Kriging (ordinary) with 10-fold cross-validation

Here is some code to enforce 10-fold cross-validation using a grouping variable that we define and select on in the for loop.  We also compare these results to the leave one out CV done above at the end of this chunk.

```{r s100 kriging for CV}
# s100 kriging for 10-fold CV --------------

# generate a grouping variable for 10-fold cross-validation
set.seed(250)
grp <- rep(1:10, length.out = 100)
grp <- sample(grp, replace = FALSE)


# Conventional Kriging, predict at the 10% of left out data
# Kriging default is OK with constant mean
# 
# First create a new dataset for the output
s100new <- list(coords = s100[[1]], data = s100[[2]], pred = numeric(length(grp)), beta = numeric(10))

# Use a loop to do the 10-fold CV:
for (i in 1:10){
    is_group <- grp == i
    training_coord <- s100[[1]][!is_group,]
    training_data <- s100[[2]][!is_group]
    valid_coord <- s100[[1]][is_group,]
    kc_cv <- krige.conv(coords = training_coord, data = training_data,
                        locations = valid_coord, 
                        krige=krige.control(obj.m=wls))
    s100new$pred[is_group] <- kc_cv$predict
    s100new$beta[i] <- kc_cv$beta.est
}

# get the properties of the kriging predictions:
mse_cv <- mean ((s100new$data - s100new$pred)^2 )
r2_cv <- 1 - mse_cv/mean((s100new$data - mean(s100new$data))^2)

#print results
mse_cv
cat("RMSE from CV:  ", sqrt(mse_cv), "\n")
r2_cv

# compare to leave-one-out CV results
# TODO:  digits in kable not working WHY?
res <- rbind(#c("  ", "MSE", "RMSE", "R2"),
      c("LOO (lik)", mse.ml, sqrt(mse.ml), r2.ml),
      c("LOO (wls)", mse.wls, sqrt(mse.wls), r2.wls),
      c("10-fold wls", mse_cv, sqrt(mse_cv), r2_cv))
kable(res, format = "markdown", col.names = c("  ", "MSE", "RMSE", "R2"), caption = "Compare LOO and 10-fold CV results", digits = 2)

```

### Universal kriging (UK)

To do universal kriging, we also need covariates for the fixed part of the model.  As discussed in Mercer et al, ArcGIS doesn't (or didn't at the time that paper was written) allow an arbitrary set of covarites to be included in UK.  It only allowed the mean function to be a function of latitude and longitude, which is far too limiting in many applications.  We demonstrate UK using `geoR` in the next section.



## Using the Snapshot data

First read in the snapshot data as a geodata object.  Summarize the data.  Take note of the range of the data coordinates, the maximum distance, and other dataset features (e.g. which covariates are included).  

Note that `geoR` appears to do *all* its distance calculations based on the input coordinates using the Pythagorean theorem.  This won't give you correct distances on a sphere.  TODO:  We still need to find a way to address the distances correctly and convert these, at least approximately, to meters.

```{r read snapshot as geodata}
# read fall snapshot as geodata --------
# focus only on the common model covariates
fall <- snapshot %>%
    filter(seasonfac == "2Fall") %>%
    select(ID, latitude, longitude, ln_nox, D2A1, A1_50, A23_400, Pop_5000, D2C, Int_3000, D2Comm, cluster, group_loc, FieldID)

fall_geo <- as.geodata(fall, coords.col = 2:3, data.col = 4, covar.col = 5:11, covar.names = names(fall)[5:11])

# summarize and get the names
names(fall_geo)
summary(fall_geo)

```

Now plot the snapshot data.  This is a simple x-y plot available in `geoR`, without any map outline.

```{r plot fall snapshot}
# plot fall snapshot -----

par(mfrow = c(1,2))
points(fall_geo, xlab = "Latitude", ylab = "Longitude", main = "Fall Snapshot Data:  ln(NOx)")

points(fall_geo, xlab = "Latitude", ylab = "Longitude", main = "Fall Snapshot Data:  ln(NOx)", pt.divide = "quintile")

```

### Mapping the data

R has many map options available now.  This seems to be evolving rapidly and there appears to be a growing number of tools available.  Some maps require an API key which adds a layer of complexity we won't address in ENVH 556.  Stamen maps do not require API keys, at least not yet.

Here is code to use `ggmap` with a Stamen map in the background.  The steps are to set the region for the map and then define the borders, call the Stamen map using a variety of options, and then overlay our data onto this.  For some basic info on using these in R, see [Getting started with Stamen maps with ggmap](https://www.r-bloggers.com/getting-started-stamen-maps-with-ggmap/).

Note:  the zoom option specifies the scaling on the map.  In `ggmap` zoom can be an integer between 1 and 21.  The smallest zooms are global and continent-level scales, the middle ones (~10-12) are city scale, and 21 is at a building level.

Note:  Maps require access to the internet to load. 

```{r LA Stamen map}
# ---------- LA Stamen map --------------
# uses ggmap; initial version of code kindly provided by Brian High
# get the basic dimensions of the data
height <- max(fall_geo$coord[,1]) - min(fall_geo$coord[,1])
width <- max(fall_geo$coord[,2]) - min(fall_geo$coord[,2])

# define the color palate for the map
#mypalette <- brewer.pal(7, "YlOrRd")

# Define the boundaries of our map -- we want it somewhat bigger than our data
# dimensions
bbox <-
    c(
        min(fall_geo$coord[,2]) - 0.1*width,
        min(fall_geo$coord[,1]) - 0.1*height,
        max(fall_geo$coord[,2]) + 0.1*width,
        max(fall_geo$coord[,1]) + 0.1*height
    )

names(bbox) <- c('left', 'bottom', 'right', 'top')

# Make a map base layer of "Stamen" tiles
map <- suppressMessages(get_stamenmap(bbox, zoom = 12,
                                     maptype = "toner-background"))

# Make the map image from the tiles and set the title
g <- ggmap(map, darken = c(0.5, "white")) + theme_void() 

# Show the map
g + ggtitle("Sample Map of Los Angeles for \n the area covered by the snapshot data")

# add snapshot locations to the map will colors for values
# Note: need to use a data.frame for this
g + geom_point(aes(x = longitude, y = latitude, col = exp(ln_nox)), data = fall, alpha = 0.8) + 
    scale_color_gradient(name = "NOx (ppb)", low = "yellow", high = "red") + 
    ggtitle("Map of Los Angeles \n with the fall snapshot data") +
    theme(legend.position = c(.98,.02), legend.justification = c(1,0)) 

```

Note, in comparing the above map to the one displayed in Figure 2b of Mercer, a few observations are in order:

* The gradient points in Mercer are spread out much more than in our map.  This was done on purpose in the displays in Mercer et al to enable the viewer to see the gradient values.  Our data have not been transformed this way.  (It would be a good exercise to implement this...)
* Not all the points on our map appear to correspond to those displayed in Mercer.  This deserves more investigation since the datasets are supposed to be identical.

Now add the grid to the map.  This isn't particularly informative, other than making it obvious to us that the area covered by the grid is not completely aligned with the area covered by the map.  (TODO:  The warning about points removed deserves investigation.)

```{r plot the grid on the map, eval = TRUE}
# plot the grid on the map ---------------
# Note: we read the grid csv data above

g + geom_point(aes(x = longitude, y = latitude), data = la_grid, alpha = 0.3) + 
    ggtitle("Map of Los Angeles \n with the grid locations overlaid") 

```


We can also compare the fall snapshot data and the grid using plots available from `geoR`.  The grid is so dense that we don't see anything but black in that plot.  Note that the y axis range in the grid plot is not the same as with the data.

```{r plot the grid}
# plot the grid ---------------
# Use the basic geoR plot

par(mfrow=c(1,2))

points(fall_geo, xlab = "Latitude", ylab = "Longitude", main = "Fall Snapshot Data:  ln(NOx)")

plot(la_grid[,2:3], xlab="Latitude", ylab = "Longitude", main = "Grid points", type = "p")

```

For later use, we need to convert the grid to geodata.  Also, the full grid isn't working, so we use a subset.

```{r convert LA grid to geodata}
# convert LA grid to geodata --------
la_geo <- as.geodata(la_grid, coords.col = 2:3, data.col = 4, covar.col = 4:10, covar.names = names(la_grid)[4:10])

test_grid <- la_grid[1:11000,]
test_geo <- as.geodata(test_grid, coords.col = 2:3, data.col = 4, covar.col = 4:10, covar.names = names(test_grid)[4:10])

```

### Estimation using the snapshot data

#### Universal kriging using the common model

**Step 1**:  Estimate the variograms and geostatistical model parameters (partial sill, range, nugget).  In the following chunk we fit two sets of models:  an OK model with no trend (i.e. covariates in a LUR), and a UK model with trend (i.e. the covariates in the common model).  We plot the variogram fits to both models for comparison.  (Note:  this comparison is for educationa/practice purposes only.  Scientifically we don't think an OK model of these data is a sensible choice.)

```{r fall estimate variog}
# fall estimate variog -----------

# get the variogram, no trend
fall_variog_notrend <- variog(fall_geo, max.dist = 0.5)

# get the variogram given the trend specified by the common model
fall_variog <- variog(fall_geo, max.dist = 0.5, trend= ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm)

# uses weighted least squares and an exponential variogram
wls_ests <- variofit(fall_variog, cov.model = "exp")
wls_ests_notrend <- variofit(fall_variog_notrend, cov.model = "exp")

# uses REML likelihood fitting and an exponential variogram
reml_ests <- likfit(fall_geo, ini = wls_ests, 
    trend = ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm, 
    cov.model = "exp", lik.method = "REML")
reml_ests_notrend <- likfit(fall_geo, ini = wls_ests_notrend,
    cov.model = "exp", lik.method = "REML")

# compare results in plots
par(mfrow = c(1,2))

# plot the variograms + estimated models
# UK model
plot(fall_variog, main="Universal kriging\nEmpirical variogram\nFall snapshot in LA")

# add fitted variogram line to the plot 
#   based on parameters from estimated variogram:
lines.variomodel(wls_ests, max.dist=0.5, lty = 1)
lines(reml_ests, lty=2, max.dist=0.5)
legend("bottomright", legend = c("WLS","REML"), lty=c(1,2), 
       cex=0.5, text.width = 0.1, seg.len = 2)

# OK model
plot(fall_variog_notrend, main="Ordinary kriging\nEmpirical variogram\nFall snapshot in LA")

# add fitted variogram line to the plot 
lines.variomodel(wls_ests_notrend, max.dist=0.5, lty = 1)
lines(reml_ests_notrend, lty=2, max.dist=0.5)
legend("bottomright", legend = c("WLS","REML"), lty=c(1,2), 
       cex=0.5, text.width = 0.1, seg.len = 2)
#   based on parameters from estimated variogram:
#lines.variomodel(cov.model="exp", cov.pars=c(.1212, 0.0747), nug=0, max.dist=0.5, lty = 1)

```

Note:  Based on the fitted variograms, I would look more into the REML estimates for UK before proceeding to use this in a real application.

Below are the estimates from the UK models, using both WLS and REML.  Note that only the REML model gives us estimates for the fixed effect parameters.  Also neither model gives us covariance parameters that match those reported by Mercer et al. TODO:  we need to address the source(s) of these differences.

```{r show fitted UK model results}
# show fitted UK model results ---------
# 
summary(wls_ests)

summary(reml_ests)

```

**Step 2**:  Cross-validate the UK models to evaluate the quality of the predictions.

TODO:  Troubleshoot.  says the trend and data for the validation locations have incompatible sizes.

```{r snapshot kriging for CV, eval = FALSE}
# ------- snapshot kriging for 10-fold CV --------------

# use the cluster variable for 10-fold cross-validation
grp <- fall$cluster

# Conventional Kriging, predict at the 10% of left out data
# Kriging default is OK with constant mean
# 
# first define the two trends
fall_trend <- trend.spatial(trend = ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm, fall_geo)

# Create a new dataset for the output
# TODO:  troubleshoot
fall_geonew <- list(coords = fall_geo[[1]], 
                    data = fall_geo[[2]], 
                    pred = numeric(length(grp)), 
                    beta = data.frame(beta0 = numeric(10),
                                          D2A1 = numeric(10),
                                          A1_50 = numeric(10),
                                          A23_400 = numeric(10),
                                          Pop_5000 = numeric(10),
                                          D2C = numeric(10),
                                          Int_3000 = numeric(10),
                                          D2Comm = numeric(10))
                    )

# Use a loop to do the 10-fold CV:
for (i in 1:10){
    is_group <- grp == i
    training_coord <- fall_geo[[1]][!is_group,]
    training_data <-  fall_geo[[2]][!is_group]
    
    valid_geo <- as.geodata(fall[is_group,], 
                            coords.col = 2:3, 
                            data.col = 4, 
                            covar.col = 5:11, 
                            covar.names = names(fall)[5:11])
    valid_coord <- valid_geo[[1]]

        test_trend <- trend.spatial(trend = ~ D2A1 + A1_50 + 
                                    A23_400 + Pop_5000 + 
                                    D2C + Int_3000 + D2Comm, 
                                valid_geo)

        kc_cv <- krige.conv(coords = training_coord, 
                            data = training_data,
                            locations = valid_coord, 
                            krige=krige.control(type = "ok",
                                        obj.m=wls_ests, 
                                        trend.d= fall_trend, 
                                        trend.l= test_trend))
    fall_geonew$pred[is_group] <- kc_cv$predict
    fall_geonew$beta[i,] <- kc_cv$beta.est
}

# get the properties of the kriging predictions:
mse_cv <- mean ((fall_geonew$data - fall_geonew$pred)^2 )
r2_cv <- 1 - mse_cv/mean((fall_geonew$data - mean(fall_geonew$data))^2)

#print results
mse_cv
cat("RMSE from CV:  ", sqrt(mse_cv), "\n")
r2_cv

# compare to leave-one-out CV results
# TODO:  digits in kable not working WHY?
res <- rbind(#c("  ", "MSE", "RMSE", "R2"),
 #     c("LOO (lik)", mse.ml, sqrt(mse.ml), r2.ml),
 #     c("LOO (wls)", mse.wls, sqrt(mse.wls), r2.wls),
      c("10-fold wls", mse_cv, sqrt(mse_cv), r2_cv))
kable(res, format = "markdown", col.names = c("  ", "MSE", "RMSE", "R2"), caption = "Compare LOO and 10-fold CV results", digits = 2)

```


**Step 3**:  Estimate the kriging predictions.  

Note:  `geoR` kriging is failing when we pass it the entire grid of >18,000 points.  It doesn't seem to fail if we use a subset of this grid with 11,000 or fewer points.  TODO:  We should find out why this is happening and correct it.

```{r krige in LA}
# krige in LA ------------
# first define the two trends
fall_trend <- trend.spatial(trend = ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm, fall_geo)

# fails under the full grid; works with a grid of 11K or smaller
#grid_trend <- trend.spatial(trend = ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm, la_geo)

test_trend <- trend.spatial(trend = ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm, test_geo)

kc_la <- krige.conv(fall_geo, 
                    locations=test_grid[,2:3],
                    krige=krige.control(type = "ok", obj.m=wls_ests, 
                                        trend.d= fall_trend, 
                                        trend.l= test_trend))


```



#### Ordinary kriging using the residuals from a LUR

TODO:  ADD this.  The idea is to use a traditional linear model to fit the common model, save the residuals from this model, and then import this model into `geoR` and fit an OK model.

### Plotting the predictions

**Step 3**:  Display the predictions on a map.  The first chunk uses the default `geoR` plots.  The second chunk adds our predictions to a Stamen map.  TODO:  We still need to figure out how to smooth these instead of plotting the individual points.

```{r fall predictions}
# fall predictions ----------

quant <- quantile(kc_la$predict, seq(0.1, 0.9, 0.1))
# force minimum to be non-negative -- matters??
breaks <- c(max(min(kc_la$predict),0),quant[c(2,4,6,8)],max(kc_la$predict))
# TODO:  fix up color range with a better range
col.range <- c("yellow","orange","red","purple","blue","black")


par(mfrow=c(1,2))
plot(fall[, 2:3],
     col = col.range[breaks],
     xlab = "Latitude",
     ylab = "Longitude")
     title(main = "Original data in ?? categories \nbased on values", line = 2)
plot(test_grid[, 2:3],
     col = col.range[breaks],
     xlab = "Latitude",
     ylab = "Longitude",
     cex = 0.1)
     title(main = "Grid predictions in ?? categories \nbased on values", line = 2)
     
```


### Variograms to compare with Mercer

TODO:  Decide if we can produce the variograms like Mercer.  Need to use the breaks function to allow for very fine scale distances to capture the gradient structure.


# Practice Session

During class we will review the output above.  Please come prepared to ask questions.

# Homework Exercises 

TODO:  update section.  Add a piece on doing UK vs. OK
TODO:  Verify what residuals for variograms in both Mercer columns
TODO:  encourage use of breaks in variogram and find the close in structure.
In addition to summarizing your geostatistical analyses, demonstrate some basic understanding of the similarities and differences between land use regression and universal kriging in air pollution studies:
1.	Describe your analysis and show some variograms.  Is there evidence of spatial structure in the data?
2.	Discuss whether you were able to replicate the results for at least one season in Table 4 of Mercer et al (2011).  (You don’t need to reproduce the table if you don’t wish to.)
3.	Write a basic summary of your understanding of how universal kriging differs from land use regression.  What additional insights do you get from the Mercer et al paper that weren’t evident in the Hoek et al paper?
4.	Discuss what are the most useful summaries to show from an exposure prediction analysis.   Base this on the papers you read this week (Mercer et al & Hoek et al) that were focused more on methods, vs. papers that are more focused on showing the results from the predictive modeling (e.g. Young et al discussed in lecture).

TODO:  from practice.  Drop??
4.	Fit a LUR model using the covariates in Table 4.  Save the residuals for further analysis.
5.	Plot a variogram of the residuals.  Explore the bandwidth and see how the variogram plots change as a function of bandwidth.  Observe similarities and differences between your plots and those presented in the paper.  (We can discuss these observations in lab.)
6.	Fit the universal kriging model.  Look at the estimates from the model and compare them with the estimates in the table.  
7.	Display the residuals from UK in a variogram; explore different bandwidths.
8.	Cross-validate the UK model.  Do you get the same performance statistics reported in Table 4?  This complements your previous cross-validation of LUR in these data.



# Code Appendix

```{r session.info}
#-----------session.info: beginning of Code Appendix -------------

sessionInfo()
```

```{r appendix.code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=T}

```


